{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:54.404384Z",
     "start_time": "2020-11-12T20:54:53.689104Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\",250)\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in SPA Input and Output Tables\n",
    "SPA input is needed in order to merge information from survey that was not written by the SPA tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:55.924012Z",
     "start_time": "2020-11-12T20:54:54.405385Z"
    }
   },
   "outputs": [],
   "source": [
    "estimation_path = r\"C:\\ABM3_dev\\estimation\"\n",
    "\n",
    "configs_dir = os.path.join(estimation_path, r\"configs\")\n",
    "\n",
    "landuse_file = r\"C:\\ABM3_dev\\run_data\\data_2z_series15\\land_use.csv\"\n",
    "landuse = pd.read_csv(landuse_file)\n",
    "\n",
    "# contains shape file\n",
    "data_dir = os.path.join(estimation_path, r\"data\\series15\\mgra15\")\n",
    "taz_dir = os.path.join(estimation_path, r\"data\\series15\\taz15\")\n",
    "\n",
    "final_output_path = r\"{dir}\\survey_data\".format(dir=estimation_path)\n",
    "\n",
    "infer_py_location = r\"{dir}\\scripts\\infer.py\".format(dir=estimation_path)\n",
    "infer_run_command = \"python \" + infer_py_location + \" \" + estimation_path + \"\\data \" + configs_dir\n",
    "\n",
    "# reading in 2016 survey\n",
    "data_16_folder = os.path.join(estimation_path, r\"data\\sandag_2016_survey\\output\")\n",
    "# reading in raw survey to be able to geocode home, school, and work locations\n",
    "raw_16_folder = os.path.join(estimation_path, r\"data\\sandag_2016_survey\\data\")\n",
    "\n",
    "# reading in 2022 survey\n",
    "data_22_folder = os.path.join(estimation_path, r\"data\\sandag_2022_survey\")\n",
    "# reading in raw survey to be able to geocode home, school, and work locations\n",
    "raw_22_folder = os.path.join(estimation_path, r\"data\\sandag_2022_survey\\sandag_hts\")\n",
    "\n",
    "# number of periods in activitysim (48 30-minute periods)\n",
    "num_periods = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maz_taz_xwalk = landuse[['MAZ', 'TAZ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz15 = geopandas.read_file(os.path.join(taz_dir, 'taz15.shp'))\n",
    "mgra15 = geopandas.read_file(os.path.join(data_dir, 'mgra15.shp'))\n",
    "# CA zone 6 to lat long\n",
    "taz15 = taz15.to_crs(epsg=4326)\n",
    "mgra15 = mgra15.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped_zones = mgra15[mgra15.ZIP.isin([92101, 92103])]\n",
    "# cropped_zones.MGRA.to_csv('downtown_zones.csv', index=False)\n",
    "# cropped_zones.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_16 = {}\n",
    "tours_16 = {}\n",
    "jtours_16 = {}\n",
    "persons_16 = {}\n",
    "households_16 = {}\n",
    "\n",
    "for day in range(1,8):\n",
    "    print(\"day\", day)\n",
    "    data_folder = os.path.join(data_16_folder, f\"day{day}\")\n",
    "    \n",
    "    households_16[day] = pd.read_csv(os.path.join(data_folder, \"households.csv\"))\n",
    "    persons_16[day] = pd.read_csv(os.path.join(data_folder, \"persons.csv\"))\n",
    "    tours_16[day] = pd.read_csv(os.path.join(data_folder, \"tours.csv\"))\n",
    "    jtours_16[day] = pd.read_csv(os.path.join(data_folder, \"unique_joint_tours.csv\"))\n",
    "    trips_16[day] = pd.read_csv(os.path.join(data_folder, \"trips.csv\"))\n",
    "\n",
    "    households_16[day]['day'] = day\n",
    "    persons_16[day]['day'] = day\n",
    "    tours_16[day]['day'] = day\n",
    "    jtours_16[day]['day'] = day\n",
    "    trips_16[day]['day'] = day\n",
    "   \n",
    "# dropping duplicate household and person records\n",
    "households_16 = pd.concat(households_16.values(), ignore_index=True)\n",
    "persons_16 = pd.concat(persons_16.values(), ignore_index=True)\n",
    "tours_16 = pd.concat(tours_16.values(), ignore_index=True)\n",
    "jtours_16 = pd.concat(jtours_16.values(), ignore_index=True)\n",
    "trips_16 = pd.concat(trips_16.values(), ignore_index=True)\n",
    "\n",
    "households_16['survey_year'] = 2016\n",
    "persons_16['survey_year'] = 2016\n",
    "tours_16['survey_year'] = 2016\n",
    "jtours_16['survey_year'] = 2016\n",
    "trips_16['survey_year'] = 2016\n",
    "\n",
    "print(\"Number of Households: \", len(households_16))\n",
    "print(\"Number of Persons: \", len(persons_16))\n",
    "print(\"Number of Joint Tours: \", len(jtours_16))\n",
    "print(\"Number of Tours: \", len(tours_16))\n",
    "print(\"Number of Trips: \", len(trips_16))\n",
    "\n",
    "raw_hh_16 = pd.read_csv(os.path.join(raw_16_folder, 'SDRTS_Household_Data_20170731.csv'))\n",
    "raw_person_16 = pd.read_csv(os.path.join(raw_16_folder, 'SDRTS_Person_Data_20170731.csv'))\n",
    "raw_vehicle_16 = pd.read_csv(os.path.join(raw_16_folder, 'SDRTS_Vehicle_Data_20170731.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_22 = {}\n",
    "tours_22 = {}\n",
    "jtours_22 = {}\n",
    "persons_22 = {}\n",
    "households_22 = {}\n",
    "\n",
    "for day in range(1,5):\n",
    "    print(\"day\", day)\n",
    "    data_folder = os.path.join(data_22_folder, \"SPA_Processed\", f\"day{day}\")\n",
    "    \n",
    "    households_22[day] = pd.read_csv(os.path.join(data_folder, \"households.csv\"))\n",
    "    persons_22[day] = pd.read_csv(os.path.join(data_folder, \"persons.csv\"))\n",
    "    tours_22[day] = pd.read_csv(os.path.join(data_folder, \"tours.csv\"))\n",
    "    jtours_22[day] = pd.read_csv(os.path.join(data_folder, \"unique_joint_tours.csv\"))\n",
    "    trips_22[day] = pd.read_csv(os.path.join(data_folder, \"trips.csv\"))\n",
    "\n",
    "    households_22[day]['day'] = day\n",
    "    persons_22[day]['day'] = day\n",
    "    tours_22[day]['day'] = day\n",
    "    jtours_22[day]['day'] = day\n",
    "    trips_22[day]['day'] = day\n",
    "   \n",
    "households_22 = pd.concat(households_22.values(), ignore_index=True)\n",
    "persons_22 = pd.concat(persons_22.values(), ignore_index=True)\n",
    "tours_22 = pd.concat(tours_22.values(), ignore_index=True)\n",
    "jtours_22 = pd.concat(jtours_22.values(), ignore_index=True)\n",
    "trips_22 = pd.concat(trips_22.values(), ignore_index=True)\n",
    "\n",
    "households_22['survey_year'] = 2022\n",
    "persons_22['survey_year'] = 2022\n",
    "tours_22['survey_year'] = 2022\n",
    "jtours_22['survey_year'] = 2022\n",
    "trips_22['survey_year'] = 2022\n",
    "\n",
    "spa_input_hhs = os.path.join(data_22_folder, \"SPA_Processed\", f\"day{day}\")\n",
    "\n",
    "print(\"Number of Households: \", len(households_22))\n",
    "print(\"Number of Persons: \", len(persons_22))\n",
    "print(\"Number of Joint Tours: \", len(jtours_22))\n",
    "print(\"Number of Tours: \", len(tours_22))\n",
    "print(\"Number of Trips: \", len(trips_22))\n",
    "\n",
    "raw_hh_22 = pd.read_csv(os.path.join(raw_22_folder, 'hh.csv'))\n",
    "raw_person_22 = pd.read_csv(os.path.join(raw_22_folder, 'person.csv'))\n",
    "raw_vehicle_22 = pd.read_csv(os.path.join(raw_22_folder, 'ex_vehicle.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_out_hh_df = pd.concat([households_16, households_22]).reset_index(drop=True)\n",
    "spa_out_per_df = pd.concat([persons_16, persons_22]).reset_index(drop=True)\n",
    "spa_out_tours_df = pd.concat([tours_16, tours_22]).reset_index(drop=True)\n",
    "spa_out_ujtours_df = pd.concat([jtours_16, jtours_22]).reset_index(drop=True)\n",
    "spa_out_trips_df = pd.concat([trips_16, trips_22]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_out_tours_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_out_trips_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process taz shp to seperate external stations\n",
    "ext_gdf = taz15[taz15['TAZ_new']<=12].copy()\n",
    "ext_gdf = ext_gdf.to_crs(4326)\n",
    "\n",
    "ext_gdf = ext_gdf.rename({'TAZ_new': 'TAZ'}, axis=1)\n",
    "\n",
    "ext_gdf.reset_index(drop=True, inplace=True)\n",
    "ext_gdf = pd.merge(ext_gdf, \n",
    "                   maz_taz_xwalk[['TAZ', 'MAZ']], \n",
    "                   on='TAZ', \n",
    "                   how='left')\n",
    "print(\"External Stations: \")\n",
    "ext_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to perfrom the tranformation every time below function is called.\n",
    "mgra15_CA = mgra15.to_crs(epsg=2230)\n",
    "ext_gdf_CA = ext_gdf.to_crs(epsg=2230)\n",
    "\n",
    "\n",
    "def geocode_to_mgra15(df, cols_to_keep, x_col, y_col):\n",
    "    # spatial join to mgra file\n",
    "    cols_to_keep.append(x_col)\n",
    "    cols_to_keep.append(y_col)\n",
    "    df_geocode = geopandas.GeoDataFrame(df[cols_to_keep], geometry=geopandas.points_from_xy(x=df[x_col], y=df[y_col]))\n",
    "    print(f\"{len(df_geocode)} entries to geocode\")\n",
    "\n",
    "    df_geocode.set_crs(epsg=4326, inplace=True)\n",
    "    assert mgra15.crs == df_geocode.crs, \"Mis-matching CRS!\"\n",
    "    df_geocode = geopandas.sjoin(mgra15[['MGRA', 'TAZ', 'geometry']], df_geocode, how='right', predicate='contains')\n",
    "\n",
    "    print(f\"\\t{df_geocode['MGRA'].isna().sum()} zones are not within the mgra15 area.\")\n",
    "    missing_xy = (df_geocode[x_col].isna() | df_geocode[y_col].isna())\n",
    "    print(f\"\\t{missing_xy.sum()} are missing x or y coordinates.\")\n",
    "\n",
    "    # assigning coastal zones\n",
    "    df_geocode['coast_geocode'] = 0\n",
    "    coastal_zones = df_geocode[\n",
    "        (df_geocode['MGRA'].isna())\n",
    "        & ~missing_xy\n",
    "        & (df_geocode.geometry.x >= -117.389) & (df_geocode.geometry.x <= -117.136)\n",
    "        & (df_geocode.geometry.y >= 32.579)& (df_geocode.geometry.y <= 33.195)\n",
    "    ]\n",
    "    # converting to geometric crs needed for nearest join, CA state plane 6\n",
    "    # need to perform after above filter\n",
    "    coastal_zones = coastal_zones.to_crs(epsg=2230)\n",
    "\n",
    "    if len(coastal_zones) > 0:\n",
    "        coastal_zones = geopandas.sjoin_nearest(mgra15_CA[['MGRA', 'TAZ', 'geometry']], coastal_zones[['geometry']], how='right')\n",
    "        df_geocode.loc[coastal_zones.index, 'MGRA'] = coastal_zones.MGRA\n",
    "        df_geocode.loc[coastal_zones.index, 'TAZ'] = coastal_zones.TAZ\n",
    "        df_geocode.loc[coastal_zones.index, 'coast_geocode'] = 1\n",
    "    print(f\"\\t{len(coastal_zones)} are coastal zones.\")\n",
    "\n",
    "    # assigning external zones\n",
    "    missing_ext_zones = df_geocode[df_geocode['MGRA'].isna() & ~missing_xy]\n",
    "    missing_ext_zones = missing_ext_zones.to_crs(epsg=2230)\n",
    "    print(f\"\\t{len(missing_ext_zones)} are assumed to be external zones.\")\n",
    "\n",
    "    df_geocode['dist_to_ext_station'] = 0\n",
    "    if len(missing_ext_zones) > 0:\n",
    "        ext_zones = geopandas.sjoin_nearest(\n",
    "            ext_gdf_CA[['TAZ', 'MAZ', 'geometry']],\n",
    "            missing_ext_zones[['geometry']],\n",
    "            how='right',\n",
    "            max_distance=360 * 5280, # within 360 miles\n",
    "            distance_col='dist_to_ext_station')\n",
    "\n",
    "        df_geocode.loc[ext_zones.index, 'MGRA'] = ext_zones.MAZ\n",
    "        df_geocode.loc[ext_zones.index, 'TAZ'] = ext_zones.TAZ\n",
    "        df_geocode.loc[ext_zones.index, 'dist_to_ext_station'] = ext_zones.dist_to_ext_station\n",
    "    \n",
    "    print(f\"\\t{(df_geocode['MGRA'].isna() & ~missing_xy).sum()} entries are outside of 360 mile buffer.\")  \n",
    "    \n",
    "    assert (df_geocode.index == df.index).all(), \"Bad Merge!\"\n",
    "    return df_geocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### home zone id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"geocoding home location\")\n",
    "hh_22_geocode = geocode_to_mgra15(raw_hh_22.set_index('hh_id'), cols_to_keep=['hh_weight'], x_col='home_lon', y_col='home_lat')\n",
    "hh_16_geocode = geocode_to_mgra15(raw_hh_16.set_index('hhid'), cols_to_keep=['hh_final_weight_456x'], x_col='home_lng', y_col='home_lat')\n",
    "\n",
    "hhid_to_home_zone_dict = hh_22_geocode['MGRA'].to_dict()\n",
    "hhid_to_home_zone_dict.update(hh_16_geocode['MGRA'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert spa_out_hh_df.HH_ID.isin(hhid_to_home_zone_dict.keys()).all()\n",
    "spa_out_hh_df['home_zone_id'] = spa_out_hh_df['HH_ID'].map(hhid_to_home_zone_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### school and work zone ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"geocoding school location:\")\n",
    "sch_22_geocode = geocode_to_mgra15(raw_person_22, cols_to_keep=['hh_id', 'person_num'], x_col='school_lon', y_col='school_lat')\n",
    "sch_22_geocode.rename(columns={'MGRA': 'school_zone_id', 'hh_id': 'HH_ID', 'person_num': 'PER_ID'}, inplace=True)\n",
    "sch_16_geocode = geocode_to_mgra15(raw_person_16, cols_to_keep=['hhid', 'pernum'], x_col='mainschool_lng', y_col='mainschool_lat')\n",
    "sch_16_geocode.rename(columns={'MGRA': 'school_zone_id', 'hhid': 'HH_ID', 'pernum': 'PER_ID'}, inplace=True)\n",
    "sch_geocode = pd.concat([sch_22_geocode, sch_16_geocode])\n",
    "sch_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"geocoding workplace location:\")\n",
    "work_22_geocode = geocode_to_mgra15(raw_person_22, cols_to_keep=['hh_id', 'person_num'], x_col='work_lon', y_col='work_lat')\n",
    "work_22_rename_dict = {'MGRA': 'work_zone_id', 'hh_id': 'HH_ID', 'person_num': 'PER_ID', 'work_lon': 'work_x', 'work_lat': 'work_y'}\n",
    "work_22_geocode.rename(columns=work_22_rename_dict, inplace=True)\n",
    "work_16_geocode = geocode_to_mgra15(raw_person_16, cols_to_keep=['hhid', 'pernum'], x_col='work_lng', y_col='work_lat')\n",
    "work_22_rename_dict = {'MGRA': 'work_zone_id', 'hh_id': 'HH_ID', 'person_num': 'PER_ID', 'work_lon': 'work_x', 'work_lat': 'work_y'}\n",
    "work_16_geocode.rename(columns={'MGRA': 'work_zone_id', 'hhid': 'HH_ID', 'pernum': 'PER_ID'}, inplace=True)\n",
    "work_geocode = pd.concat([work_22_geocode, work_16_geocode])\n",
    "work_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_index = spa_out_per_df.index\n",
    "school_work_geocode = pd.concat([sch_geocode[['HH_ID', 'PER_ID', 'school_zone_id']], work_geocode[['work_zone_id']]], axis=1)\n",
    "spa_out_per_merge_df = pd.merge(spa_out_per_df, school_work_geocode, how='left', on=['HH_ID', 'PER_ID'], suffixes=('', ''))\n",
    "pd.testing.assert_index_equal(spa_out_per_merge_df.index, orig_index)\n",
    "spa_out_per_df[['school_zone_id', 'work_zone_id']] = spa_out_per_merge_df[['school_zone_id', 'work_zone_id']].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for end in ['ORIG', 'DEST']:\n",
    "    print(f\"geocoding tour {end}:\")\n",
    "    tour_geocode_df = geocode_to_mgra15(spa_out_tours_df, cols_to_keep=['HH_ID', 'PER_ID', 'TOUR_ID'], x_col=end + '_X', y_col=end + '_Y')\n",
    "    assert (tour_geocode_df.index == spa_out_tours_df.index).all(), \"Bad Merge!\"\n",
    "    spa_out_tours_df[end + '_MAZ'] = tour_geocode_df['MGRA']\n",
    "    spa_out_tours_df[end + '_TAZ'] = tour_geocode_df['TAZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for end in ['ORIG', 'DEST']:\n",
    "    trip_geocode_df = geocode_to_mgra15(spa_out_trips_df, cols_to_keep=['HH_ID', 'PER_ID', 'TOUR_ID', 'TRIP_ID'], x_col=end + '_X', y_col=end + '_Y')\n",
    "    assert (trip_geocode_df.index == spa_out_trips_df.index).all(), \"Bad Merge!\"\n",
    "    spa_out_trips_df[end + '_MAZ'] = trip_geocode_df['MGRA']\n",
    "    spa_out_trips_df[end + '_TAZ'] = trip_geocode_df['TAZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# from folium import plugins\n",
    "\n",
    "# heat_data = [[point.xy[1][0], point.xy[0][0]] for point in trip_geocode_df.geometry]\n",
    "\n",
    "# map = folium.Map(location=[32.95, -116.897], tiles=\"OpenStreetMap\", zoom_start=9, height='100%', width='100%')\n",
    "\n",
    "# plugins.HeatMap(heat_data, min_opacity=0.2, radius=12).add_to(map)\n",
    "\n",
    "# map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(series1, series2):\n",
    "    result = series1.reindex(series2)\n",
    "    try:\n",
    "        result.index = series2.index\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing tours and trips involving coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining functions used to identify and assign nearest MAZ/TAZ\n",
    "###\n",
    "def get_dist(a,b,c,d):\n",
    "    R = 3958.8\n",
    "\n",
    "    if type(c) != float:\n",
    "        a = np.repeat(a, len(c))\n",
    "        b = np.repeat(b, len(c))\n",
    "    \n",
    "    lat1 = np.deg2rad(a)\n",
    "    lon1 = np.deg2rad(b)\n",
    "    lat2 = np.deg2rad(c)\n",
    "    lon2 = np.deg2rad(d)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = (np.sin(dlat/2))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(dlon/2))**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    distance = R * c\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing tours and trips with external stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify trips whose destination is not tour's primary destination but external \n",
    "spa_out_trips_df['external_dest_is_stop'] = 0\n",
    "spa_out_trips_df.loc[spa_out_trips_df['DEST_TAZ'].isin(ext_gdf['TAZ'])&\n",
    "                     (spa_out_trips_df['DEST_IS_TOUR_DEST']==0), 'external_dest_is_stop'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the group of tours having an external stop\n",
    "temp_ext_df = spa_out_trips_df[spa_out_trips_df['external_dest_is_stop']==1]\n",
    "temp_ext_df = temp_ext_df.groupby(['HH_ID', 'PER_ID', 'TOUR_ID', 'day']).size().reset_index().rename(columns={0: 'has_external_stop'})\n",
    "temp_ext_df['has_external_stop'] = 1\n",
    "temp_ext_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the column identifying if a tour has external stop or not\n",
    "spa_out_tours_df = pd.merge(spa_out_tours_df, \n",
    "                            temp_ext_df,\n",
    "                            how='left',\n",
    "                            on=['HH_ID', 'PER_ID', 'TOUR_ID', 'day'], \n",
    "                            suffixes=('','_x'))\n",
    "spa_out_tours_df['has_external_stop'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add tour origin coordinates to each trip\n",
    "spa_out_trips_merge_df = pd.merge(spa_out_trips_df, \n",
    "                            spa_out_tours_df[['HH_ID', 'PER_ID', 'TOUR_ID', 'day', 'ORIG_X', 'ORIG_Y']],\n",
    "                            how='left',\n",
    "                            on=['HH_ID', 'PER_ID', 'TOUR_ID', 'day'], \n",
    "                            suffixes=('','_TOUR'))\n",
    "\n",
    "### Calculate distance b/w tour origin and trip destinations\n",
    "spa_out_trips_df['dest_dist'] = spa_out_trips_merge_df.apply(lambda x: \n",
    "    (get_dist(a=x.DEST_Y, b=x.DEST_X, c=x.ORIG_Y_TOUR, d=x.ORIG_X_TOUR)), axis=1)\n",
    "\n",
    "### Trips that are further than 360 miles from tour origin are open jaw tours\n",
    "spa_out_trips_df['IE_open_jaw'] = 0\n",
    "spa_out_trips_df.loc[spa_out_trips_df['dest_dist']>360, 'IE_open_jaw'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the group of tours having an external stop\n",
    "temp_open_jaw_df = spa_out_trips_df[spa_out_trips_df['IE_open_jaw']==1]\n",
    "temp_open_jaw_df = temp_open_jaw_df.groupby(['HH_ID', 'PER_ID', 'TOUR_ID', 'day']).size().reset_index().rename(columns={0: 'IE_open_jaw'})\n",
    "temp_open_jaw_df['IE_open_jaw'] = 1\n",
    "temp_open_jaw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the column identifying if a tour is open jawed or not\n",
    "spa_out_tours_df = pd.merge(spa_out_tours_df,\n",
    "                            temp_open_jaw_df,\n",
    "                            how='left',\n",
    "                            on=['HH_ID', 'PER_ID', 'TOUR_ID', 'day'], \n",
    "                            suffixes=('','_x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coding external tour origins\n",
    "spa_out_tours_df['OTAZ_ext'] = 0\n",
    "spa_out_tours_df.loc[spa_out_tours_df['ORIG_TAZ'].isin(ext_gdf['TAZ']), 'OTAZ_ext'] = 1\n",
    "\n",
    "### Coding external tour destinations\n",
    "spa_out_tours_df['DTAZ_ext'] = 0\n",
    "spa_out_tours_df.loc[spa_out_tours_df['DEST_TAZ'].isin(ext_gdf['TAZ']), 'DTAZ_ext'] = 1\n",
    "\n",
    "'''\n",
    "External tour types:\n",
    "    II - Fully internal tour\n",
    "    II-Ext - Starts and ends internally but whose primary destination is external (these could also have external stops)\n",
    "    II-Ext(internal_dest) - Tours that start and end internally with an internal primary destination but at least one external stop\n",
    "    EE - Fully external tour\n",
    "'''\n",
    "spa_out_tours_df['external_type'] = np.nan\n",
    "spa_out_tours_df.loc[(spa_out_tours_df['OTAZ_ext']==0), 'external_type'] = 'II'\n",
    "spa_out_tours_df.loc[(spa_out_tours_df['OTAZ_ext']==0)&\n",
    "                     (spa_out_tours_df['DTAZ_ext']==1), 'external_type'] = 'II-Ext' \n",
    "spa_out_tours_df.loc[(spa_out_tours_df['OTAZ_ext']==0)&\n",
    "                     (spa_out_tours_df['DTAZ_ext']==0)&\n",
    "                     (spa_out_tours_df['has_external_stop']==1), 'external_type'] = 'II-Ext(internal_dest)'\n",
    "spa_out_tours_df.loc[(spa_out_tours_df['IE_open_jaw']==1), 'external_type'] = 'IE-Open_Jaw'\n",
    "spa_out_tours_df.loc[(spa_out_tours_df['OTAZ_ext']==1), 'external_type'] = 'EE'\n",
    "spa_out_tours_df['external_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Household File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spa output household file just selects SAMPN and HH_SIZE and renames to HH_ID and NUM_PERS.  AREA is not used.  So, processing this file requires just changing the variables for the input hh file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:56.215326Z",
     "start_time": "2020-11-12T20:54:56.211323Z"
    }
   },
   "outputs": [],
   "source": [
    "hh_inc_cat_dict_22 = {\n",
    "    1: [0,14999],\n",
    "    2: [15000,24999],\n",
    "    3: [25000,34999],\n",
    "    4: [35000,49999],\n",
    "    5: [50000,74999],\n",
    "    6: [75000,99999],\n",
    "    7: [100000,149999],\n",
    "    8: [150000,199999],\n",
    "    9: [200000,249999],\n",
    "    10: [249999,300000], # 250k and up\n",
    "    999: [80000, 85000], # median income in san diego is about $83,000\n",
    "}\n",
    "\n",
    "hh_inc_cat_dict_16 = {\n",
    "    1: [0,14999],\n",
    "    2: [15000,29999],\n",
    "    3: [30000,44999],\n",
    "    4: [45000,59999],\n",
    "    5: [60000,74999],\n",
    "    6: [75000,99999],\n",
    "    7: [100000,129999],\n",
    "    8: [125000,149999],\n",
    "    9: [150000,199999],\n",
    "    10: [200000,249999],\n",
    "    11: [249999,300000], # 250k and up\n",
    "    99: [80000, 85000], # median income in san diego is about $83,000\n",
    "}\n",
    "\n",
    "def interpolate_hh_income(row):\n",
    "    inc_cat = row['HH_INC_CAT']\n",
    "    if inc_cat < 0:\n",
    "        return pd.NA\n",
    "    else:\n",
    "        if row['survey_year'] == 2022:\n",
    "            return np.random.randint(hh_inc_cat_dict_22[inc_cat][0], hh_inc_cat_dict_22[inc_cat][1])\n",
    "        else:\n",
    "            # converting 2016 dollars to 2022\n",
    "            # $1 in 2016 is worth $1.25 in 2022 (https://www.bls.gov/data/inflation_calculator.htm)\n",
    "            return np.random.randint(hh_inc_cat_dict_22[inc_cat][0], hh_inc_cat_dict_22[inc_cat][1]) * 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:56.274375Z",
     "start_time": "2020-11-12T20:54:56.216334Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_hh_df = pd.DataFrame()\n",
    "\n",
    "asim_hh_df['HH_ID'] = spa_out_hh_df['HH_ID']\n",
    "asim_hh_df['home_zone_id'] = spa_out_hh_df['home_zone_id']\n",
    "asim_hh_df['survey_year'] = spa_out_hh_df['survey_year']\n",
    "asim_hh_df['hhsize'] = spa_out_per_df[['HH_ID', 'PER_ID']].drop_duplicates().groupby('HH_ID')['PER_ID'].count().reindex(asim_hh_df.HH_ID, fill_value=0).values\n",
    "asim_hh_df['day'] = spa_out_hh_df['day']\n",
    "asim_hh_df['num_workers'] = spa_out_per_df[spa_out_per_df['PERSONTYPE'].isin([1,2])][['HH_ID', 'PER_ID']].drop_duplicates().groupby('HH_ID')['PER_ID'].count().reindex(asim_hh_df.HH_ID, fill_value=0).values\n",
    "asim_hh_df['auto_ownership'] = pd.concat([raw_hh_22.set_index('hh_id')['num_vehicles'], raw_hh_16.set_index('hhid')['vehicle_count']]).reindex(asim_hh_df.HH_ID).values\n",
    "asim_hh_df['auto_ownership'].clip(upper=4, inplace=True)\n",
    "asim_hh_df['HH_INC_CAT'] = pd.concat([raw_hh_22.set_index('hh_id')['income_detailed'], raw_hh_16.set_index('hhid')['hhincome_imputed']]).reindex(asim_hh_df.HH_ID).fillna(999).values\n",
    "assert ~(asim_hh_df[['num_workers', 'auto_ownership', 'HH_INC_CAT']].isna()).any().any()\n",
    "\n",
    "res_type_coding_dict = { # mapping 2022 survey codes to 2016 codes\n",
    "    1: 1, # single family detatched --> single family detatched\n",
    "    2: 2, # single family attached --> single family attached\n",
    "    3: 3, # 2-4 units --> 3 or fewer units\n",
    "    4: 4, # 5-49 units --> 4 or more units\n",
    "    5: 4, # 50+ units  --> 4 or more units\n",
    "    6: 6, # senior / age restricted --> dorm / barracks / institutional housing\n",
    "    7: 5, # mobile home  --> mobile home\n",
    "    8: 6, # dorm / GQ / institutional housing --> dorm / barracks / institutional housing\n",
    "    9: 97, # other --> other\n",
    "    995: 97, # missing --> other\n",
    "}\n",
    "asim_hh_df['res_type'] = pd.concat(\n",
    "    [raw_hh_22.set_index('hh_id')['res_type'].map(res_type_coding_dict),\n",
    "     raw_hh_16.set_index('hhid')['res_type']]\n",
    ").reindex(asim_hh_df.HH_ID).fillna(999).values\n",
    "\n",
    "asim_hh_df['bldgsz'] = np.select(\n",
    "    # mobile home, single family detached, single family attached, 20-49 apartments, 50+ apartments\n",
    "    [asim_hh_df['res_type'] == 5, asim_hh_df['res_type'] == 1,  asim_hh_df['res_type'] == 2, asim_hh_df['res_type'].isin([3,4]), asim_hh_df['res_type'].isin([6,97])],\n",
    "    [1, 2, 3, 8, 9]\n",
    ")\n",
    "# HHT set below after creation of persons table\n",
    "\n",
    "# Linear interpolation of income categories, and sample from distribution for missing income values\n",
    "asim_hh_df['income'] = asim_hh_df.apply(lambda row: interpolate_hh_income(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_hh_df.bldgsz.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing toll transponder ownership from vehicles table\n",
    "# household is said to own a transponder if at least one vehicle has a transponder\n",
    "raw_vehicle_16['has_toll_transponder'] = np.where(raw_vehicle_16['tolltransp'] == 2, 1, 0)\n",
    "tr_own_16 = raw_vehicle_16.groupby('hhid')['has_toll_transponder'].sum().clip(upper=1)\n",
    "tr_own_22 = raw_vehicle_22.groupby('hh_id')['toll_transponder'].sum().clip(upper=1)\n",
    "tr_own = pd.concat([tr_own_16, tr_own_22]).reindex(asim_hh_df.HH_ID).fillna(0)\n",
    "asim_hh_df['transponder_ownership'] = tr_own.astype(bool).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_hh_df.transponder_ownership.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:56.487104Z",
     "start_time": "2020-11-12T20:54:56.275376Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_hh_df['income'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_hh_df['auto_ownership'].clip(upper=4).value_counts(dropna=False, normalize=True).loc[[0,1,2,3,4]].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_hh_df.loc[asim_hh_df['HH_ID'].drop_duplicates().index, 'auto_ownership'].clip(upper=4).value_counts(dropna=False, normalize=True).loc[[0,1,2,3,4]].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Person File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022 & 2016 surveys had different age categories\n",
    "age_cat_dict_22 = {\n",
    "    1: [0, 5],\n",
    "    2: [5, 15],\n",
    "    3: [16, 17],\n",
    "    4: [18, 24],\n",
    "    5: [25, 34],\n",
    "    6: [35, 44],\n",
    "    7: [45, 54],\n",
    "    8: [55, 64],\n",
    "    9: [65, 74],\n",
    "    10: [75, 84],\n",
    "    11: [85, 90], # 85 and up\n",
    "}\n",
    "\n",
    "age_cat_dict_16 = {\n",
    "    1: [0, 4],\n",
    "    2: [5, 15],\n",
    "    3: [16, 17],\n",
    "    4: [18, 24],\n",
    "    5: [25, 34],\n",
    "    6: [35, 44],\n",
    "    7: [45, 49],\n",
    "    8: [50, 54],\n",
    "    9: [55, 59],\n",
    "    10: [60, 64],\n",
    "    11: [65, 74],\n",
    "    12: [75, 79],\n",
    "    13: [80, 84],\n",
    "    14: [85, 90], # 85 and up\n",
    "}\n",
    "\n",
    "def interpolate_age(row):\n",
    "    age_cat = row['AGE_CAT']\n",
    "    if (age_cat > 0) & (age_cat < 14):\n",
    "        if row['survey_year'] == 2016:\n",
    "            return np.random.randint(age_cat_dict_16[row['AGE_CAT']][0], age_cat_dict_16[row['AGE_CAT']][1] + 1)  # [low,high)\n",
    "        else:\n",
    "            return np.random.randint(age_cat_dict_22[row['AGE_CAT']][0], age_cat_dict_22[row['AGE_CAT']][1] + 1)\n",
    "    \n",
    "    # impute age based on Student and employment category\n",
    "    if row['pstudent'] == 1: # school\n",
    "        return 13 # non-driving age student\n",
    "    elif row['pstudent'] == 2:  # university\n",
    "        return 20\n",
    "    else:\n",
    "        return 45 # generic adult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:56.571695Z",
     "start_time": "2020-11-12T20:54:56.516649Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_per_df = pd.DataFrame()\n",
    "\n",
    "keep_cols = ['HH_ID', 'PER_ID', 'day', 'survey_year']\n",
    "\n",
    "asim_per_df[keep_cols] = spa_out_per_df[keep_cols]\n",
    "asim_per_df['ptype'] = spa_out_per_df['PERSONTYPE']\n",
    "asim_per_df['pstudent'] = spa_out_per_df['STU_CAT']\n",
    "asim_per_df['is_student'] = asim_per_df['pstudent'].isin([1, 2]) # school or university\n",
    "asim_per_df['pemploy'] = spa_out_per_df['EMP_CAT']\n",
    "asim_per_df['AGE_CAT'] = spa_out_per_df['AGE_CAT'].where(~spa_out_per_df['AGE_CAT'].isna(), spa_out_per_df['AGE'], axis=0).fillna(999)\n",
    "asim_per_df['age'] = asim_per_df.apply(lambda row: interpolate_age(row), axis=1)\n",
    "asim_per_df['PNUM'] = spa_out_per_df['PER_ID']\n",
    "\n",
    "# looks like some persontypes weren't coded correctly if age is missing\n",
    "asim_per_df.loc[asim_per_df['ptype'].isna() & (asim_per_df['pstudent'] == 1), 'ptype'] = 7  # school, assumed non-driving age\n",
    "asim_per_df.loc[asim_per_df['ptype'].isna() & (asim_per_df['pstudent'] == 2), 'ptype'] = 3  # univ student\n",
    "asim_per_df.loc[asim_per_df['ptype'].isna() & (asim_per_df['pstudent'] == 3) & (asim_per_df['pemploy'] == 4), 'ptype'] = 4  # non-worker\n",
    "asim_per_df['ptype'] = asim_per_df['ptype'].astype(int)\n",
    "\n",
    "asim_per_df['school_zone_id'] = spa_out_per_df['school_zone_id']\n",
    "asim_per_df['workplace_zone_id'] = spa_out_per_df['work_zone_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merging other data from raw table that was not available in SPA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex = 1 for male and 2 for female in ActivitySim configs & synthetic population\n",
    "raw_person_22['sex'] = np.where(raw_person_22['gender'] == 2, 1, 2) # if gender is 2: Male, then Male, else Female (2: Female, 3: transgender, 4: non-binary, 999: other)\n",
    "raw_person_16['sex'] = np.where(raw_person_16['gender'] == 1, 1, 2) # if gender is 1: Male, then Male, else Female (2: Female, other)\n",
    "\n",
    "raw_person_22['work_from_home'] = (raw_person_22['job_type'] == 3) # work only from home or remotely\n",
    "raw_person_16['work_from_home'] = (raw_person_16['job_type'] == 3) # work only from home or remotely\n",
    "\n",
    "raw_person_22['transit_pass_subsidy'] = np.where(raw_person_22['commute_subsidy_3'] == 1, 1, 0) # Commute Benefits Provided by Employer: Free/discount transit fare\n",
    "raw_person_16['transit_pass_subsidy'] = np.where(raw_person_16['commute_subsidy_transit'] == 1, 1, 0) # Commute Subsidy: Free/subsidized transit fare (yes/no)\n",
    "\n",
    "raw_person_22['transit_pass_ownership'] = np.where(raw_person_22['transit_pass_ownership'] == 1, 1, 0) # Has a PRONTO card\n",
    "raw_person_16['transit_pass_ownership'] = np.where(raw_person_16['transitpass'].isin(range(1,14)), 1, 0) # 1-13 is transit pass variant, 14 is no, 98 is don't know\n",
    "\n",
    "# commute_subsidy_1 asks whether Employer provides free partking at work, 1: yes, 0: no, other: N/A\n",
    "raw_person_22['free_parking_at_work'] = np.where(raw_person_22['commute_subsidy_1'] == 1, True, False)\n",
    "# 1: no cost, 2: Employer pays all parking, other includes discounts, paying for pass, full cost, and N/A\n",
    "raw_person_16['free_parking_at_work'] = np.where(raw_person_16['work_park_pay'].isin([1,2]), True, False) \n",
    "\n",
    "telecommute_freq_dict_22 = {\n",
    "    996: 'No_Telecommute', # never\n",
    "    995: 'No_Telecommute', # missing\n",
    "    1: '4_days_week', # 6-7 days a week\n",
    "    2: '2_3_days_week', # 5 days a week\n",
    "    3: '2_3_days_week', # 4 days a week\n",
    "    4: '4_days_week', # 2-3 days a week\n",
    "    5: '1_day_week', # 1 day a week\n",
    "    6: '1_day_week', # 1-3 days a month\n",
    "    7: 'No_Telecommute', # less than monthly\n",
    "}\n",
    "raw_person_22['telecommute_frequency'] = raw_person_22['telework_freq'].map(telecommute_freq_dict_22)\n",
    "telecommute_freq_dict_16 = {\n",
    "    1: '4_days_week', # 6-7 days a week\n",
    "    2: '4_days_week', # 5 days a week\n",
    "    3: '2_3_days_week', # 4 days a week\n",
    "    4: '2_3_days_week', # 2-3 days a week\n",
    "    5: '1_day_week', # 1 day a week\n",
    "    6: '1_day_week', # 9 days every 2 weeks\n",
    "    7: '1_day_week', # 1-3 days a month\n",
    "    8: 'No_Telecommute', # less than monthly\n",
    "    9: 'No_Telecommute', # never\n",
    "}\n",
    "raw_person_16['telecommute_frequency'] = raw_person_16['telecommute_freq'].fillna(9).map(telecommute_freq_dict_16)\n",
    "\n",
    "# merge into asim_per_df table\n",
    "raw_person_16['HH_ID'] = raw_person_16['hhid']\n",
    "raw_person_16['PER_ID'] = raw_person_16['pernum']\n",
    "raw_person_22['HH_ID'] = raw_person_22['hh_id']\n",
    "raw_person_22['PER_ID'] = raw_person_22['person_num']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industry Coding\n",
    "# new ABM coding (source: Grace email to Joel on 30Mar23 and forwarded to David on 4Apr23)\n",
    "#[Govn't, utility/manufacturing/wholesale, military, agriculture/mining, business services, Finacnce/insurance/real estate/mgmt enterprises,\n",
    "# Education, Healthcare, Retail Trade, Entertainment, Accommodation, Food Services, Construction/transporation/warehousing, other services,\n",
    "# non-wage/salary WFH, non-wage / salary non-wfh]\n",
    "# putting in pd.NA for no response and N/A\n",
    "\n",
    "# FIXME: hard to separate food and accomodation, where to put fitness?\n",
    "industry_coding_dict_2022 = {\n",
    "    1: 'entertainment', # Arts and entertainment\n",
    "    2: 'other', # Childcare (e.g., nanny, babysitter)\n",
    "    3: 'construction', # Construction or landscaping\n",
    "    4: 'education', # Education (public or private)\n",
    "    5: 'government', # Government\n",
    "    6: 'mgmt_srv', # Financial services\n",
    "    7: 'healthcare', # Health care\n",
    "    8: 'accomodation', # Hospitality (e.g., restaurant, accommodation)\n",
    "    9: 'manufacturing', # Manufacturing (e.g., aerospace & defense, electrical, machinery)\n",
    "    10: 'entertainment', # Media\n",
    "    11: 'military', # Military\n",
    "    12: 'agriculture', # Natural resources (e.g., forestry, fisher, energy)\n",
    "    13: 'business_srv', # Professional and business services (e.g., consulting, legal, marketing)\n",
    "    14: 'retail', # Personal services (e.g., hair styling, personal assistance, pet sitting)\n",
    "    15: 'mgmt_srv', # Real estate\n",
    "    16: 'retail', # Retail\n",
    "    17: 'other', # Social assistance\n",
    "    18: 'other', # Sports and fitness\n",
    "    19: 'business_srv', # Technology and telecommunications\n",
    "    20: 'construction', # Transportation and utilities\n",
    "    997: 'other', # Other\n",
    "    995: np.nan, # Missing Response\n",
    "}\n",
    "raw_person_22['industry_coded'] = raw_person_22['industry'].map(industry_coding_dict_2022)\n",
    "\n",
    "# FIXME: no government category\n",
    "industry_coding_dict_2016 = {\n",
    "    1: 'accomodation', # Accommodation (e.g., hotels/motels)\n",
    "    2: 'mgmt_srv', # Administrative, Support, & Waste Management Services\n",
    "    3: 'agriculture', # Agriculture, Forestry, Fishing, & Hunting\n",
    "    4: 'entertainment', # Arts, Entertainment, & Recreation\n",
    "    5: 'construction', # Construction\n",
    "    6: 'education', # Education Services\n",
    "    7: 'food_srv', # Food Services & Drinking Places\n",
    "    8: 'mgmt_srv', # Finance & Insurance\n",
    "    9: 'healthcare', # Health Care & Social Assistance\n",
    "    10: 'business_srv', # Information\n",
    "    11: 'mgmt_srv', # Management of Companies & Enterprises\n",
    "    12: 'manufacturing', # Manufacturing\n",
    "    13: 'military', # Military\n",
    "    14: 'agriculture', # Mining, Quarrying, & Oil/Gas Extraction\n",
    "    15: 'other', # Other Services\n",
    "    16: 'business_srv', # Professional, Scientific, & Technical Services\n",
    "    17: 'mgmt_srv', # Public Administration\n",
    "    18: 'mgmt_srv', # Real Estate, Rental, & Leasing\n",
    "    19: 'retail', # Retail Trade\n",
    "    20: 'construction', # Transportation & Warehousing\n",
    "    21: 'construction', # Utilities\n",
    "    22: 'construction', # Wholesale Trade\n",
    "    97: 'other', # Other\n",
    "    98: np.nan, # Don't Know\n",
    "}\n",
    "raw_person_16['industry_coded'] = raw_person_16['industry'].map(industry_coding_dict_2016)\n",
    "\n",
    "merge_cols = ['HH_ID', 'PER_ID', 'work_from_home', 'telecommute_frequency', 'sex',\n",
    "              'free_parking_at_work', 'transit_pass_subsidy', 'transit_pass_ownership', 'industry_coded', 'relationship']\n",
    "raw_persons = pd.concat([raw_person_22[merge_cols], raw_person_16[merge_cols]])\n",
    "\n",
    "asim_per_df = asim_per_df.merge(raw_persons, how='left', on=['HH_ID', 'PER_ID'])\n",
    "asim_per_df.rename(columns={'industry_coded': 'industry'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship coding: \n",
    "# 0: Self\n",
    "# 1: Spouse/partner\n",
    "# 2: Son/daughter/in-law\n",
    "# 3: Father/mother/in-law\n",
    "# 4: Brother/sister/in-law\n",
    "# 5: Other relative (e.g., grandchild, cousin)\n",
    "# 6: Roommate/friend\n",
    "# 7: Household help\n",
    "# 997 or 97: Other\n",
    "\n",
    "# HHT coding logic\n",
    "# Person 1 | HH size | Anyone identifies as spouse of p1? | Anyone identifies as roommate/help of person 1 | HHT | Description\n",
    "# Male | 1 | NA | NA | 4 | 4 .Nonfamily household: Male householder: Living alone\n",
    "# Female | 1 | NA | NA | 6 | 6 .Nonfamily household: Female householder: Living alone\n",
    "# Male | 2 | yes | NA | 1 | 1 .Married couple household\n",
    "# Female | 2 | yes | NA | 1 | 1 .Married couple household\n",
    "# Male | >1 | no | No | 2 | 2 .Other family household: Male householder, no spouse present\n",
    "# Female | >1 | no | No | 3 | 3 .Other family household: Female householder, no spouse present\n",
    "# Male | >1 | no | yes | 5  | 5 .Nonfamily household: Male householder: Not living alone\n",
    "# Female | >1 | no | yes | 7 | 7 .Nonfamily household: Female householder: Not living alone\n",
    "\n",
    "def determine_hht(grp):\n",
    "    per_1_sex = grp.loc[grp['PNUM'] == 1, 'sex'].values[0]\n",
    "    male_head = (per_1_sex == 1)\n",
    "    hh_id = grp.HH_ID.values[0]\n",
    "    # hhsize = asim_hh_df.loc[asim_hh_df['HH_ID'] == hh_id, 'hhsize'].values[0]\n",
    "    hhsize = grp.PER_ID.nunique()\n",
    "\n",
    "    # display(grp)\n",
    "    if grp.relationship.isin([7]).any():\n",
    "        if ~grp.relationship.isin([1]).any():\n",
    "            print(\"here\")\n",
    "\n",
    "    if male_head & (hhsize == 1):\n",
    "        hht = 4 # non-family household, male householder, living alone\n",
    "    elif ~male_head & (hhsize == 1):\n",
    "        hht = 6 # non-family household, female householder, living alone\n",
    "    elif (hhsize >= 2) & grp.relationship.isin([1]).any():\n",
    "        hht = 1 # married couple household\n",
    "    elif male_head & ~(grp.relationship.isin([1,7]).any()):\n",
    "        hht = 2 # other family household, male householder, no spouse\n",
    "    elif ~male_head & ~(grp.relationship.isin([1,7]).any()):\n",
    "        hht = 3 # other family household, female householder, no spouse\n",
    "    elif male_head & ~grp.relationship.isin([1]).any() & grp.relationship.isin([7]).any():\n",
    "        hht = 5 # non-family household, male householder, not living alone\n",
    "    elif ~male_head & ~grp.relationship.isin([1]).any() & grp.relationship.isin([7]).any():\n",
    "        hht = 7 # non-family household, female householder, not living alone\n",
    "    else:\n",
    "        raise RuntimeError(\"Bad HHT coding for group: \", grp[['HH_ID', 'sex', 'PNUM', 'relationship']], hhsize, male_head)\n",
    "    # print(f\"HHT: {hht}, hhsize: {hhsize}, hhid: {hh_id}, male head?: {male_head}, 1 in rel? {grp.relationship.isin([1]).any()}\")\n",
    "    return hht\n",
    "\n",
    "hht_df = asim_per_df.groupby('HH_ID').apply(lambda grp: determine_hht(grp))\n",
    "asim_hh_df['HHT'] = asim_hh_df.HH_ID.map(hht_df.to_dict())\n",
    "asim_hh_df['HHT'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(asim_per_df['telecommute_frequency'], asim_per_df['survey_year'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(asim_per_df['work_from_home'], asim_per_df['survey_year'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(asim_per_df['sex'], asim_per_df['survey_year'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(asim_per_df['industry'], asim_per_df['survey_year'], dropna=False, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Tours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tour mode coding in SPA tool:\n",
    "    * 'SOV': 1,\n",
    "    * 'HOV2': 2,\n",
    "    * 'HOV3': 3,\n",
    "    * 'WALK': 4,\n",
    "    * 'BIKE': 5,\n",
    "    * 'WALK-TRANSIT': 6,\n",
    "    * 'PNR-TRANSIT': 7,\n",
    "    * 'KNR-TRANSIT': 8,\n",
    "    * 'TNC-TRANSIT': 9,\n",
    "    * 'TAXI': 10,\n",
    "    * 'TNC-REG': 11,\n",
    "    * 'TNC-POOL': 12,\n",
    "    * 'SCHOOLBUS': 13,\n",
    "    * 'OTHER': 14\n",
    "    \n",
    "Tour purpose coding in SPA tool:\n",
    "    * 'HOME':         0,\n",
    "    * 'WORK':         1,\n",
    "    * 'UNIVERSITY':   2,\n",
    "    * 'SCHOOL':       3,\n",
    "    * 'ESCORTING':    4,\n",
    "    * 'SHOPPING':     5,\n",
    "    * 'MAINTENANCE':  6,\n",
    "    * 'EAT OUT':      7,\n",
    "    * 'SOCIAL/VISIT': 8,\n",
    "    * 'DISCRETIONARY':9,\n",
    "    * 'WORK-RELATED': 10,\n",
    "    * 'LOOP':         11,\n",
    "    * 'CHANGE MODE':  12,\n",
    "    * 'OTHER':        13\n",
    "\n",
    "JOINT_STATUS coding in spa tool:\n",
    "* (1) independent tours:        all trips.JOINT==NOT-JOINT\n",
    "* (2) partially-joint tours:    all trips.JOINT<>JOINT; some are FULLY-JOINT, some are NOT-JOINT\n",
    "* (3) fully-joint tours:        tour.get_is_fully_joint()==True\n",
    "* (4) partially-joint problematic tours: some trips are NOT-JOINT, some are JOINT (not grouped)\n",
    "* (5) jointness-unclear tours :    no NOT-JOINT, some are JOINT\n",
    "\n",
    "In ActivitySim, JOINT_STATUS == 3 are considered joint tours. Only fully-joint tours have joint_tour_participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:56.638775Z",
     "start_time": "2020-11-12T20:54:56.634771Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2016 survey was also recoded to use these modes\n",
    "tour_mode_spa_to_asim_dict_22 = {\n",
    "    1: 'SOV',\n",
    "    2: 'HOV2',\n",
    "    3: 'HOV3',\n",
    "    4: 'WALK',\n",
    "    5: 'BIKE',\n",
    "    6: 'WALK_LOC',\n",
    "    7: 'WALK_PRM',\n",
    "    8: 'WALK_MIX',\n",
    "    9: 'PNR_LOC',\n",
    "    10: 'PNR_PRM',\n",
    "    11: 'PNR_MIX',\n",
    "    12: 'KNR_LOC',\n",
    "    13: 'KNR_PRM',\n",
    "    14: 'KNR_MIX',\n",
    "    15: 'TNC_LOC',\n",
    "    16: 'TNC_PRM',\n",
    "    17: 'TNC_MIX',\n",
    "    18: 'TNC_SINGLE',\n",
    "    19: 'TNC_SHARED',\n",
    "    20: 'TAXI',\n",
    "    21: 'SCH_BUS',\n",
    "    22: 'SOV', # other to SOV\n",
    "}\n",
    "\n",
    "tour_purpose_spa_to_asim_dict = {\n",
    "    0: 'home', # used for trips\n",
    "    1: 'work',\n",
    "    2: 'univ',\n",
    "    3: 'school',\n",
    "    4: 'escort',\n",
    "    5: 'shopping',\n",
    "    6: 'othmaint',\n",
    "    7: 'eatout',\n",
    "    8: 'social',\n",
    "    9: 'othdiscr',\n",
    "    10: 'othmaint',    # work-related, no counts in this category\n",
    "    11: 'othdiscr',    # Loop\n",
    "    12: 'othdiscr',    # Change mode \n",
    "    13: 'othdiscr',    # other\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:57.670293Z",
     "start_time": "2020-11-12T20:54:56.654788Z"
    }
   },
   "outputs": [],
   "source": [
    "# determining parent tour purpose for subtours\n",
    "spa_out_tours_df = pd.merge(\n",
    "    spa_out_tours_df,\n",
    "    spa_out_tours_df[['HH_ID','PER_ID','TOUR_ID', 'TOURPURP', 'day']],\n",
    "    how='left',\n",
    "    left_on=['HH_ID', 'PER_ID', 'PARENT_TOUR_ID', 'day'],\n",
    "    right_on=['HH_ID','PER_ID','TOUR_ID', 'day'],\n",
    "    suffixes=('','_y')\n",
    ")\n",
    "spa_out_tours_df.drop(columns='TOUR_ID_y', inplace=True)\n",
    "spa_out_tours_df.rename(columns={'TOURPURP_y':'PARENT_TOURPURP'}, inplace=True)\n",
    "spa_out_tours_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:58.132289Z",
     "start_time": "2020-11-12T20:54:58.021172Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df = spa_out_tours_df[\n",
    "    ['HH_ID', 'PER_ID','TOUR_ID', 'PARENT_TOUR_ID', 'JTOUR_ID', 'day', 'external_type',\n",
    "     'OUT_ESCORT_TYPE', 'OUT_CHAUFFUER_ID', 'OUT_CHAUFFUER_PURP',\n",
    "     'INB_ESCORT_TYPE', 'INB_CHAUFFUER_ID', 'INB_CHAUFFUER_PURP',\n",
    "     'OUT_ESCORTING_TYPE', 'INB_ESCORTING_TYPE', 'OUT_ESCORTEE_TOUR_PURP', 'INB_ESCORTEE_TOUR_PURP']].copy()\n",
    "\n",
    "asim_tour_df['origin'] = spa_out_tours_df['ORIG_MAZ']\n",
    "asim_tour_df['destination'] = spa_out_tours_df['DEST_MAZ']\n",
    "asim_tour_df['survey_year'] = spa_out_tours_df['survey_year']\n",
    "\n",
    "asim_tour_df['start'] = spa_out_tours_df['ANCHOR_DEPART_BIN']\n",
    "asim_tour_df['end'] = spa_out_tours_df['ANCHOR_ARRIVE_BIN']\n",
    "asim_tour_df['duration'] = asim_tour_df['end'] - asim_tour_df['start']\n",
    "\n",
    "# treating joint tours as only those that have a JTOUR_ID\n",
    "asim_tour_df['is_joint'] = spa_out_tours_df['JTOUR_ID'].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "asim_tour_df['is_subtour'] = spa_out_tours_df['IS_SUBTOUR']\n",
    "\n",
    "asim_tour_df.loc[asim_tour_df['start'] > num_periods, 'start'] = asim_tour_df.loc[asim_tour_df['start'] > num_periods, 'start'] - num_periods\n",
    "asim_tour_df.loc[asim_tour_df['end'] > num_periods, 'end'] = asim_tour_df.loc[asim_tour_df['end'] > num_periods, 'end'] - num_periods\n",
    "\n",
    "asim_tour_df.loc[asim_tour_df['survey_year'] == 2016, 'tour_mode'] = spa_out_tours_df.loc[\n",
    "    asim_tour_df['survey_year'] == 2016, 'TOURMODE'].map(tour_mode_spa_to_asim_dict_22)\n",
    "asim_tour_df.loc[asim_tour_df['survey_year'] == 2022, 'tour_mode'] = spa_out_tours_df.loc[\n",
    "    asim_tour_df['survey_year'] == 2022, 'TOURMODE'].map(tour_mode_spa_to_asim_dict_22)\n",
    "\n",
    "asim_tour_df['tour_purpose'] = spa_out_tours_df['TOURPURP'].map(tour_purpose_spa_to_asim_dict)\n",
    "asim_tour_df.loc[asim_tour_df['is_subtour'] == 1, 'parent_tour_purpose'] = spa_out_tours_df.loc[\n",
    "    asim_tour_df['is_subtour'] == 1, 'PARENT_TOURPURP'].map(tour_purpose_spa_to_asim_dict)\n",
    "\n",
    "assert (~asim_tour_df['tour_mode'].isna()).all(), \"Missing tour modes!\"\n",
    "assert (~asim_tour_df['tour_purpose'].isna()).all(), \"Missing tour purpose!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:58.141297Z",
     "start_time": "2020-11-12T20:54:58.133290Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df.loc[asim_tour_df['is_subtour'] == 1, 'parent_tour_purpose'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.190872Z",
     "start_time": "2020-11-12T20:54:58.142297Z"
    }
   },
   "outputs": [],
   "source": [
    "def determine_tour_category(row):\n",
    "    if (row['is_subtour'] == 1) & (row['parent_tour_purpose'] == 'work'):\n",
    "        return 'atwork'\n",
    "    elif row['tour_purpose'] in ['work', 'univ', 'school']:\n",
    "        return 'mandatory'\n",
    "    elif row['is_joint'] == 1:\n",
    "        return 'joint'\n",
    "    else:\n",
    "        return 'non_mandatory'\n",
    "\n",
    "def determine_tour_type(row):\n",
    "    if row['tour_category'] == 'atwork':\n",
    "        if row['tour_purpose'] == 'work':\n",
    "            return 'business'\n",
    "        elif row['tour_purpose'] == 'eatout':\n",
    "            return 'eat'\n",
    "        else:\n",
    "            return 'maint'\n",
    "    else:\n",
    "        return row['tour_purpose']\n",
    "    \n",
    "asim_tour_df['tour_category'] = asim_tour_df.apply(lambda row: determine_tour_category(row), axis=1)\n",
    "asim_tour_df['tour_type'] = asim_tour_df.apply(lambda row: determine_tour_type(row), axis=1)\n",
    "# asim_tour_df.loc[asim_tour_df['tour_category'] == 'atwork', 'tour_purpose'] = 'atwork'  # has to be after tour_type calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_tour_df['tour_category'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(asim_tour_df['tour_type'], asim_tour_df['tour_category'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.404115Z",
     "start_time": "2020-11-12T20:54:59.227925Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df['duration'].hist(bins=np.linspace(0,48,48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Tour Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.426139Z",
     "start_time": "2020-11-12T20:54:59.415130Z"
    }
   },
   "outputs": [],
   "source": [
    "spa_out_ujtours_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.435147Z",
     "start_time": "2020-11-12T20:54:59.427140Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df = pd.melt(spa_out_ujtours_df,\n",
    "       id_vars=['HH_ID', 'JTOUR_ID', 'day'],\n",
    "       value_vars=['PERSON_1','PERSON_2','PERSON_3','PERSON_4','PERSON_5','PERSON_6','PERSON_7','PERSON_8','PERSON_9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.444154Z",
     "start_time": "2020-11-12T20:54:59.436147Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df = asim_jtour_participants_df[pd.notna(asim_jtour_participants_df['value'])]\n",
    "asim_jtour_participants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.452160Z",
     "start_time": "2020-11-12T20:54:59.445155Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df[asim_jtour_participants_df['HH_ID'] == 70004828]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.461168Z",
     "start_time": "2020-11-12T20:54:59.453161Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df['participant_num'] = asim_jtour_participants_df['variable'].apply(lambda x: int(x.strip('PERSON_')))\n",
    "asim_jtour_participants_df['PER_ID'] = asim_jtour_participants_df['value'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.474185Z",
     "start_time": "2020-11-12T20:54:59.462169Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Indexing\n",
    "Need unique household_id, per_id, tour_id, etc. for ActivitySim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.488190Z",
     "start_time": "2020-11-12T20:54:59.475180Z"
    }
   },
   "outputs": [],
   "source": [
    "# household ID should be unique already, but we want to be sure\n",
    "asim_hh_df['household_id'] = asim_hh_df.reset_index().index + 1\n",
    "asim_hh_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.511234Z",
     "start_time": "2020-11-12T20:54:59.489191Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_per_df = pd.merge(\n",
    "    asim_per_df,\n",
    "    asim_hh_df[['HH_ID', 'household_id', 'day']],\n",
    "    how='left',\n",
    "    on=['HH_ID', 'day'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.529249Z",
     "start_time": "2020-11-12T20:54:59.512235Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_per_df['person_id'] = asim_per_df.reset_index().index + 1\n",
    "asim_per_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to re-number PNUM\n",
    "# not every person is listed for every day in spa output.  See HH_ID == 22008078 for an example.\n",
    "# There is a cut in infer looking for PNUM == 1, which every household needs.\n",
    "asim_per_df['PNUM'] = asim_per_df.groupby('household_id')['person_id'].cumcount() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining number of children in each household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_children = asim_per_df[asim_per_df['age'] < 18].groupby('household_id')['person_id'].count().to_frame()\n",
    "hh_children.columns = ['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_hh_df.set_index('household_id', inplace=True)\n",
    "asim_hh_df.loc[hh_children.index, 'children'] = hh_children['children']\n",
    "asim_hh_df['children'] = asim_hh_df['children'].fillna(0).astype(int)\n",
    "asim_hh_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.543261Z",
     "start_time": "2020-11-12T20:54:59.530250Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.575287Z",
     "start_time": "2020-11-12T20:54:59.544262Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df = pd.merge(\n",
    "    asim_tour_df,\n",
    "    asim_per_df[['day', 'HH_ID', 'PER_ID', 'household_id', 'person_id']],\n",
    "    how='left',\n",
    "    on=['HH_ID', 'PER_ID', 'day']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint tours are replicated in SPA output accross all members of the tour.  asim_tour_df will keep just the first instance of joint tours.  Members of the joint tours are tracked in the asim_jtour_participants_df table.\n",
    "\n",
    "Not removing duplicated joint tours will cause different tour_id's to be assigned to the same joint tour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.611342Z",
     "start_time": "2020-11-12T20:54:59.576288Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df.sort_values(by=['day', 'HH_ID', 'JTOUR_ID', 'PER_ID', 'TOUR_ID',], inplace = True)\n",
    "asim_tour_df['prev_JTOUR_ID'] = asim_tour_df['JTOUR_ID'].shift(1)\n",
    "asim_tour_df['prev_HH_ID'] = asim_tour_df['HH_ID'].shift(1)\n",
    "same_household = (asim_tour_df['HH_ID'] == asim_tour_df['prev_HH_ID'])\n",
    "same_jtour = ((asim_tour_df['prev_JTOUR_ID'] == asim_tour_df['JTOUR_ID']) & asim_tour_df['JTOUR_ID'].notna())\n",
    "asim_tour_df['is_duplicated_jtour'] = 0\n",
    "asim_tour_df.loc[(same_household & same_jtour), 'is_duplicated_jtour'] = 1\n",
    "asim_tour_df['hh_duplicated_jtours'] = asim_tour_df.groupby(['HH_ID'])['is_duplicated_jtour'].transform('sum')\n",
    "asim_tour_df.sort_values(by=['day', 'HH_ID', 'PER_ID', 'TOUR_ID'], inplace = True)\n",
    "all_asim_tour_df = asim_tour_df.copy()\n",
    "asim_tour_df = asim_tour_df[asim_tour_df['is_duplicated_jtour'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.689406Z",
     "start_time": "2020-11-12T20:54:59.612343Z"
    }
   },
   "outputs": [],
   "source": [
    "#asim_tour_df['survey_tour_id'] = asim_tour_df['TOUR_ID']\n",
    "#asim_tour_df['survey_person_id'] = asim_tour_df['PER_ID']\n",
    "asim_tour_df['tour_id'] = asim_tour_df.reset_index().index + 1\n",
    "\n",
    "# merge parent_tour_id\n",
    "asim_tour_df = pd.merge(\n",
    "    asim_tour_df,\n",
    "    asim_tour_df[['HH_ID', 'PER_ID', 'TOUR_ID', 'tour_id', 'day']],\n",
    "    how='left',\n",
    "    left_on=['HH_ID', 'PER_ID', 'PARENT_TOUR_ID', 'day'],\n",
    "    right_on=['HH_ID', 'PER_ID', 'TOUR_ID', 'day'],\n",
    "    suffixes=('','_y')\n",
    ")\n",
    "asim_tour_df.drop(columns='TOUR_ID_y', inplace=True)\n",
    "asim_tour_df.rename(columns={'tour_id_y':'parent_tour_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.718455Z",
     "start_time": "2020-11-12T20:54:59.709449Z"
    }
   },
   "outputs": [],
   "source": [
    "# do not allow subtours that are not joint from joint tours\n",
    "asim_tour_df = asim_tour_df[~(asim_tour_df['PARENT_TOUR_ID'].notna() & asim_tour_df['parent_tour_id'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint Tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.750482Z",
     "start_time": "2020-11-12T20:54:59.742475Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.777504Z",
     "start_time": "2020-11-12T20:54:59.751483Z"
    }
   },
   "outputs": [],
   "source": [
    "# merging person_id's separately since not every person may be listed in tour_file\n",
    "asim_jtour_participants_df = pd.merge(\n",
    "    asim_jtour_participants_df,\n",
    "    asim_per_df[['HH_ID', 'PER_ID', 'day', 'household_id', 'person_id']],\n",
    "    how='left',\n",
    "    on=['HH_ID', 'PER_ID', 'day']\n",
    ")\n",
    "\n",
    "# merging tour_id \n",
    "asim_jtour_participants_df = pd.merge(\n",
    "    asim_jtour_participants_df,\n",
    "    asim_tour_df[['HH_ID', 'JTOUR_ID', 'tour_id', 'day']],\n",
    "    how='left',\n",
    "    on=['HH_ID', 'JTOUR_ID', 'day'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.795026Z",
     "start_time": "2020-11-12T20:54:59.778505Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df.sort_values(by=['household_id', 'tour_id', 'participant_num'], inplace=True)\n",
    "asim_jtour_participants_df['participant_id'] = asim_jtour_participants_df.reset_index().index + 1\n",
    "asim_jtour_participants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.801539Z",
     "start_time": "2020-11-12T20:54:59.796528Z"
    }
   },
   "outputs": [],
   "source": [
    "all(asim_jtour_participants_df['tour_id'].isin(asim_tour_df['tour_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional changes to make infer.py work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"univ\" is not a trip option\n",
    "converting all univ trips to school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.812558Z",
     "start_time": "2020-11-12T20:54:59.802542Z"
    }
   },
   "outputs": [],
   "source": [
    "num_univ_tours = len(asim_tour_df[asim_tour_df['tour_type'] == 'univ'])\n",
    "print(\"Number of univ tours: \", num_univ_tours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.819564Z",
     "start_time": "2020-11-12T20:54:59.813559Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df.loc[asim_tour_df['tour_type'] == 'univ', 'tour_type'] = \"school\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No joint escorting mode\n",
    "error: Unable to parse string \"j_escort1\"\n",
    "\n",
    "solution: recategorizing joint escort tours to non_mandatory escort tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.829572Z",
     "start_time": "2020-11-12T20:54:59.820565Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df[asim_tour_df['tour_type'] == 'escort']['tour_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.835577Z",
     "start_time": "2020-11-12T20:54:59.830573Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df.loc[asim_tour_df['tour_type'] == 'escort', 'tour_category'] = 'non_mandatory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.844585Z",
     "start_time": "2020-11-12T20:54:59.836578Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df[asim_tour_df['tour_type'] == 'escort']['tour_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing tours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tours with invalid start or end MAZ's are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.869605Z",
     "start_time": "2020-11-12T20:54:59.845586Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_num_tours = len(asim_tour_df)\n",
    "asim_tour_df = asim_tour_df[asim_tour_df['origin'].isin(landuse.mgra)]\n",
    "asim_tour_df = asim_tour_df[asim_tour_df['destination'].isin(landuse.mgra)]\n",
    "valid_maz_tours = len(asim_tour_df)\n",
    "print(\"Removed\", orig_num_tours - valid_maz_tours, \"tours due to invalid tour start or end maz\")\n",
    "print(valid_maz_tours, \" tours remain\")\n",
    "\n",
    "# removing these tours from the joint_tour_participants file\n",
    "asim_jtour_participants_df = asim_jtour_participants_df[asim_jtour_participants_df['tour_id'].isin(asim_tour_df['tour_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.880614Z",
     "start_time": "2020-11-12T20:54:59.870606Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.888621Z",
     "start_time": "2020-11-12T20:54:59.881616Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_jtour_participants_df['tot_num_participants'] = asim_jtour_participants_df.groupby('tour_id')['participant_num'].transform('max')\n",
    "asim_jtour_participants_df['tot_num_participants'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using configs files to determine allowed tour frequencies\n",
    "For example, each person can only have 2 mandatory tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.900645Z",
     "start_time": "2020-11-12T20:54:59.889622Z"
    }
   },
   "outputs": [],
   "source": [
    "mand_tour_freq_alts_df = pd.read_csv(\n",
    "    os.path.join(configs_dir, 'mandatory_tour_frequency_alternatives.csv'),\n",
    "    comment=\"#\")\n",
    "non_mand_tour_freq_alts_df = pd.read_csv(\n",
    "    os.path.join(configs_dir, 'non_mandatory_tour_frequency_alternatives.csv'),\n",
    "    comment=\"#\")\n",
    "non_mand_tour_freq_alts_df['alt'] = non_mand_tour_freq_alts_df.index\n",
    "joint_tour_freq_alts_df = pd.read_csv(\n",
    "    os.path.join(configs_dir, 'joint_tour_frequency_alternatives.csv'),\n",
    "    comment=\"#\")\n",
    "atwork_subtour_freq_alts_df = pd.read_csv(\n",
    "    os.path.join(configs_dir, 'atwork_subtour_frequency_alternatives.csv'),\n",
    "    comment=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:54:59.905656Z",
     "start_time": "2020-11-12T20:54:59.901647Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_tours_by_category(df, category, count_by, tour_types):\n",
    "    for tour_type in tour_types:\n",
    "        count_name = category + \"_\" + tour_type\n",
    "        print(tour_type)\n",
    "        df[count_name] = \\\n",
    "            df[(df['tour_category'] == category) & (df['tour_type'] == tour_type)] \\\n",
    "            .groupby(count_by).cumcount() + 1\n",
    "        df.loc[(df[count_by] != df.shift(1)[count_by]) & pd.isna(df[count_name]), count_name] = 0\n",
    "        df[count_name].ffill(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.033792Z",
     "start_time": "2020-11-12T20:54:59.907159Z"
    }
   },
   "outputs": [],
   "source": [
    "# - checking mandatory tour frequency\n",
    "mand_tour_types = list(mand_tour_freq_alts_df.columns.drop('alt'))\n",
    "asim_tour_df = count_tours_by_category(\n",
    "    df=asim_tour_df,\n",
    "    category='mandatory',\n",
    "    count_by='person_id',\n",
    "    tour_types=mand_tour_types\n",
    ")\n",
    "\n",
    "asim_tour_df = pd.merge(\n",
    "    asim_tour_df,\n",
    "    mand_tour_freq_alts_df,\n",
    "    how='left',\n",
    "    right_on=mand_tour_types,\n",
    "    left_on=['mandatory_' + tour_type for tour_type in mand_tour_types]\n",
    ")\n",
    "asim_tour_df.drop(labels=mand_tour_types, axis='columns', inplace=True)\n",
    "asim_tour_df.rename(columns={'alt': 'mandatory_alt'}, inplace=True)\n",
    "asim_tour_df[asim_tour_df['tour_category'] == 'mandatory']['mandatory_alt'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.054810Z",
     "start_time": "2020-11-12T20:55:00.034793Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df['mandatory_school'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.325109Z",
     "start_time": "2020-11-12T20:55:00.055810Z"
    }
   },
   "outputs": [],
   "source": [
    "# - checking non_mandatory tour frequency\n",
    "non_mand_tour_types = list(non_mand_tour_freq_alts_df.columns.drop('alt'))\n",
    "asim_tour_df = count_tours_by_category(\n",
    "    df=asim_tour_df,\n",
    "    category='non_mandatory',\n",
    "    count_by='person_id',\n",
    "    tour_types=non_mand_tour_types\n",
    ")\n",
    "\n",
    "asim_tour_df = pd.merge(\n",
    "    asim_tour_df,\n",
    "    non_mand_tour_freq_alts_df,\n",
    "    how='left',\n",
    "    right_on=non_mand_tour_types,\n",
    "    left_on=['non_mandatory_' + tour_type for tour_type in non_mand_tour_types]\n",
    ")\n",
    "asim_tour_df.drop(labels=non_mand_tour_types, axis='columns', inplace=True)\n",
    "asim_tour_df.rename(columns={'alt': 'non_mandatory_alt'}, inplace=True)\n",
    "asim_tour_df[asim_tour_df['tour_category'] == 'non_mandatory']['non_mandatory_alt'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.506305Z",
     "start_time": "2020-11-12T20:55:00.326110Z"
    }
   },
   "outputs": [],
   "source": [
    "# - checking atwork tour frequency\n",
    "atwork_subtour_types = list(atwork_subtour_freq_alts_df.columns.drop('alt'))\n",
    "asim_tour_df = count_tours_by_category(\n",
    "    df=asim_tour_df,\n",
    "    category='atwork',\n",
    "    count_by='parent_tour_id',\n",
    "    tour_types=atwork_subtour_types\n",
    ")\n",
    "\n",
    "asim_tour_df = pd.merge(\n",
    "    asim_tour_df,\n",
    "    atwork_subtour_freq_alts_df,\n",
    "    how='left',\n",
    "    right_on=atwork_subtour_types,\n",
    "    left_on=['atwork_' + tour_type for tour_type in atwork_subtour_types]\n",
    ")\n",
    "asim_tour_df.drop(labels=atwork_subtour_types, axis='columns', inplace=True)\n",
    "asim_tour_df.rename(columns={'alt': 'atwork_alt'}, inplace=True)\n",
    "asim_tour_df[asim_tour_df['tour_category'] == 'atwork']['atwork_alt'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.515317Z",
     "start_time": "2020-11-12T20:55:00.507808Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df[asim_tour_df['tour_category'] == 'atwork']['tour_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.805628Z",
     "start_time": "2020-11-12T20:55:00.520321Z"
    }
   },
   "outputs": [],
   "source": [
    "# - checking joint tour frequency\n",
    "# FIXME need to modify to match new joint_tour_frequency_composition model\n",
    "joint_tour_types = list(joint_tour_freq_alts_df.columns.drop('alt'))\n",
    "asim_tour_df = count_tours_by_category(\n",
    "    df=asim_tour_df,\n",
    "    category='joint',\n",
    "    count_by='household_id',\n",
    "    tour_types=joint_tour_types\n",
    ")\n",
    "\n",
    "# merging joint tour alternatives\n",
    "asim_tour_df = pd.merge(\n",
    "    asim_tour_df,\n",
    "    joint_tour_freq_alts_df,\n",
    "    how='left',\n",
    "    right_on=joint_tour_types,\n",
    "    left_on=['joint_' + tour_type for tour_type in joint_tour_types]\n",
    ")\n",
    "asim_tour_df.drop(labels=joint_tour_types, axis='columns', inplace=True)\n",
    "asim_tour_df.rename(columns={'alt': 'joint_alt'}, inplace=True)\n",
    "asim_tour_df[asim_tour_df['tour_category'] == 'joint']['joint_alt'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.845667Z",
     "start_time": "2020-11-12T20:55:00.808133Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df['keep_tour'] = 1\n",
    "original_num_tours = len(asim_tour_df)\n",
    "\n",
    "asim_tour_df.loc[(asim_tour_df['tour_category'] == 'mandatory') & pd.isna(asim_tour_df['mandatory_alt']), 'keep_tour'] = 0\n",
    "asim_tour_df.loc[(asim_tour_df['tour_category'] == 'non_mandatory') & pd.isna(asim_tour_df['non_mandatory_alt']), 'keep_tour'] = 0\n",
    "asim_tour_df.loc[(asim_tour_df['tour_category'] == 'atwork') & pd.isna(asim_tour_df['atwork_alt']), 'keep_tour'] = 0\n",
    "asim_tour_df.loc[(asim_tour_df['tour_category'] == 'joint') & pd.isna(asim_tour_df['joint_alt']), 'keep_tour'] = 0\n",
    "\n",
    "after_removed_tours = len(asim_tour_df[asim_tour_df['keep_tour'] == 1])\n",
    "print(\"Removed \", original_num_tours - after_removed_tours, \"tours that did not match in the tour frequency configs files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not allow \n",
    "original_num_tours = asim_tour_df['keep_tour'].sum()\n",
    "\n",
    "asim_tour_df.loc[(asim_tour_df['is_subtour'] == 1) & (asim_tour_df['parent_tour_purpose'] != 'work'), 'keep_tour'] = 0\n",
    "\n",
    "after_removed_tours = asim_tour_df['keep_tour'].sum()\n",
    "print(\"Removed \", original_num_tours - after_removed_tours, \"subtours without a parent work purpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Marking tours that could be part of extension model\n",
    "# FIXME: should be grabbing this from configs\n",
    "asim_tour_df['included_by_extension_model'] = 0\n",
    "asim_tour_df.loc[(asim_tour_df['keep_tour']==0)&\n",
    "                 (asim_tour_df['tour_category']=='non_mandatory')&\n",
    "                 (asim_tour_df['non_mandatory_escort']<=4)&\n",
    "                 (asim_tour_df['non_mandatory_shopping']<=4)&\n",
    "                 (asim_tour_df['non_mandatory_othmaint']<=4)&\n",
    "                 (asim_tour_df['non_mandatory_othdiscr']<=4)&\n",
    "                 (asim_tour_df['non_mandatory_eatout']<=3)&\n",
    "                 (asim_tour_df['non_mandatory_social']<=3), 'included_by_extension_model'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_tour_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If person takes a work tour, they can't work from home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_taking_work_tours = asim_tour_df.loc[asim_tour_df['tour_purpose'] == 'work', 'person_id']\n",
    "wfh_and_work_tour = asim_per_df.loc[asim_per_df['work_from_home'] & asim_per_df['person_id'].isin(people_taking_work_tours), 'person_id']\n",
    "print(f\"Changing work from home from True to False for {len(wfh_and_work_tour)} people that take a work tour\")\n",
    "asim_per_df.loc[asim_per_df['person_id'].isin(wfh_and_work_tour), 'work_from_home'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tour Start and End times must be acceptable\n",
    "Checking the tour_departure_and_duration_alternatives.csv configs file for allowable times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.852683Z",
     "start_time": "2020-11-12T20:55:00.846668Z"
    }
   },
   "outputs": [],
   "source": [
    "tdd_df = pd.read_csv(os.path.join(configs_dir, \"tour_departure_and_duration_alternatives.csv\"))\n",
    "min_start_allowed = tdd_df['start'].min()\n",
    "max_start_allowed = tdd_df['start'].max()\n",
    "min_end_allowed = tdd_df['end'].min()\n",
    "max_end_allowed = tdd_df['end'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.883699Z",
     "start_time": "2020-11-12T20:55:00.853673Z"
    }
   },
   "outputs": [],
   "source": [
    "count_before_tdd = len(asim_tour_df[asim_tour_df['keep_tour'] == 1])\n",
    "asim_tour_df.loc[\n",
    "    (asim_tour_df['start'] < min_start_allowed)\n",
    "    | (asim_tour_df['start'] > max_start_allowed)\n",
    "    | (asim_tour_df['end'] < min_end_allowed)\n",
    "    | (asim_tour_df['end'] > max_end_allowed)\n",
    "    | pd.isna(asim_tour_df['start'])\n",
    "    | pd.isna(asim_tour_df['end'])\n",
    "    | (asim_tour_df['start'] > asim_tour_df['end']), 'keep_tour'] = 0\n",
    "\n",
    "count_after_tdd = len(asim_tour_df[asim_tour_df['keep_tour'] == 1])\n",
    "\n",
    "print(\"Removed an additional\", count_before_tdd - count_after_tdd, \"tours due to bad start/end times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reassigning tours from persons that make work or school trip but have invalid work or school MAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.898220Z",
     "start_time": "2020-11-12T20:55:00.890705Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_per_df.loc[(asim_per_df['school_zone_id'].isin([0,-9999])) | (asim_per_df['school_zone_id'].isna()), 'school_zone_id'] = -1\n",
    "asim_per_df.loc[(asim_per_df['workplace_zone_id'].isin([0,-9999])) | (asim_per_df['workplace_zone_id'].isna()), 'workplace_zone_id'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.918754Z",
     "start_time": "2020-11-12T20:55:00.899724Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_per_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.925759Z",
     "start_time": "2020-11-12T20:55:00.919754Z"
    }
   },
   "outputs": [],
   "source": [
    "univ_students = (asim_per_df['pstudent'] == 2)\n",
    "gradeschool_students = ((asim_per_df['pstudent'] == 1) & (asim_per_df['age'] < 14))\n",
    "highschool_students = ((asim_per_df['pstudent'] == 1) & (asim_per_df['age'] >= 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.935767Z",
     "start_time": "2020-11-12T20:55:00.926760Z"
    }
   },
   "outputs": [],
   "source": [
    "landuse['tot_college_enroll'] = landuse['collegeenroll'] + landuse['othercollegeenroll'] + landuse['adultschenrl']\n",
    "univ_mazs = landuse[landuse['tot_college_enroll'] > 0]['mgra']\n",
    "k_8_mazs = landuse[landuse['enrollgradekto8'] > 0]['mgra']\n",
    "G9_12_mazs = landuse[landuse['enrollgrade9to12'] > 0]['mgra']\n",
    "print(len(univ_mazs), 'MAZs with university enrollment')\n",
    "print(len(k_8_mazs), 'MAZs with K-8 enrollment')\n",
    "print(len(G9_12_mazs), 'MAZs with 9-12 enrollment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.970797Z",
     "start_time": "2020-11-12T20:55:00.936769Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_school_co(asim_per_df):\n",
    "    school_co = asim_per_df[univ_students | gradeschool_students | highschool_students].copy()\n",
    "    school_co['school_segment_named'] = 'university'\n",
    "    school_co.loc[highschool_students, 'school_segment_named'] = 'highschool'\n",
    "    school_co.loc[gradeschool_students, 'school_segment_named'] = 'gradeschool'\n",
    "    school_co['has_landuse_maz'] = 0\n",
    "    school_co.loc[(univ_students & (school_co['school_zone_id'].isin(univ_mazs)))\n",
    "                  | (highschool_students & (school_co['school_zone_id'].isin(G9_12_mazs)))\n",
    "                  | (gradeschool_students & (school_co['school_zone_id'].isin(k_8_mazs))),\n",
    "                'has_landuse_maz'] = 1\n",
    "    return school_co\n",
    "school_co = make_school_co(asim_per_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:00.991814Z",
     "start_time": "2020-11-12T20:55:00.971797Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_missing_school_maz_df(school_co):\n",
    "    missing_maz_df = pd.DataFrame({\n",
    "        'survey_total': school_co['school_segment_named'].value_counts(),\n",
    "        'survey_missing_maz': school_co[school_co['school_zone_id'] == -1]['school_segment_named'].value_counts(),\n",
    "        'has_valid_maz': school_co[school_co['has_landuse_maz'] == 1]['school_segment_named'].value_counts()\n",
    "    })\n",
    "    missing_maz_df.loc['total'] = missing_maz_df.sum()\n",
    "    missing_maz_df['percent_missing'] = missing_maz_df['survey_missing_maz'] / missing_maz_df['survey_total'] * 100\n",
    "    missing_maz_df['percent_valid'] = missing_maz_df['has_valid_maz'] / missing_maz_df['survey_total'] * 100\n",
    "    return missing_maz_df\n",
    "\n",
    "make_missing_school_maz_df(school_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:01.019862Z",
     "start_time": "2020-11-12T20:55:00.992815Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_school_co = school_co[school_co['has_landuse_maz'] == 0]\n",
    "bad_school_co_by_maz = bad_school_co.groupby(['school_zone_id', 'school_segment_named']).count().reset_index()\n",
    "bad_school_co_by_maz = bad_school_co_by_maz.pivot_table(index='school_zone_id', columns='school_segment_named', values='person_id')\n",
    "bad_school_co_by_maz = bad_school_co_by_maz.reset_index()\n",
    "bad_school_co_by_maz = bad_school_co_by_maz.rename(columns={'school_zone_id': 'MAZ'}).fillna(0)\n",
    "bad_school_co_by_maz = bad_school_co_by_maz[bad_school_co_by_maz['MAZ'] > 0]\n",
    "bad_school_co_by_maz\n",
    "# bad_school_co_by_maz.astype(int).to_csv('reported_school_locations_without_enrollment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:01.683461Z",
     "start_time": "2020-11-12T20:55:01.024866Z"
    }
   },
   "outputs": [],
   "source": [
    "maz_shp_landuse = pd.merge(mgra15, landuse, how='left', left_on='MGRA', right_on='mgra')\n",
    "# maz_shp_landuse = geopandas.GeoDataFrame(maz_shp_landuse, geometry='geometry', crs=mgra15.crs)\n",
    "maz_shp_landuse = maz_shp_landuse.to_crs(epsg=2230)  # CA state plane 6 (feet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:55:01.703001Z",
     "start_time": "2020-11-12T20:55:01.684965Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_school_shp = pd.merge(bad_school_co_by_maz, mgra15, how='left', left_on='MAZ', right_on='MGRA')\n",
    "bad_school_shp = geopandas.GeoDataFrame(bad_school_shp, geometry='geometry', crs=mgra15.crs)\n",
    "# ignore external zones\n",
    "bad_school_shp = bad_school_shp[~bad_school_shp['geometry'].isna()]\n",
    "bad_school_shp = bad_school_shp.to_crs(epsg=2230)  # CA state plane 6 (feet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:46.634437Z",
     "start_time": "2020-11-12T20:55:01.704001Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_closest_valid_maz(row, maz_shp_landuse):\n",
    "    if (row.MAZ < 0) | pd.isna(row.MAZ):\n",
    "        return row\n",
    "    centroid = row['geometry']\n",
    "    \n",
    "    if row.gradeschool > 0:\n",
    "        gradeschool_mazs = maz_shp_landuse[maz_shp_landuse['enrollgradekto8'] > 0].reset_index(drop=True)\n",
    "        distances = [centroid.distance(geom) for geom in gradeschool_mazs['geometry']]\n",
    "        row['closest_gradeschool_distance'] = np.amin(distances) / 5280 # ft to miles\n",
    "        row['closest_gradeschool_maz'] = gradeschool_mazs.loc[np.argmin(distances), 'mgra']\n",
    "        \n",
    "    if row.highschool > 0:\n",
    "        highschool_mazs = maz_shp_landuse[maz_shp_landuse['enrollgrade9to12'] > 0].reset_index(drop=True)\n",
    "        distances = [centroid.distance(geom) for geom in highschool_mazs['geometry']]\n",
    "        row['closest_highschool_distance'] = np.amin(distances) / 5280 # ft to miles\n",
    "        row['closest_highschool_maz'] = highschool_mazs.loc[np.argmin(distances), 'mgra']\n",
    "    \n",
    "    if row.university > 0:\n",
    "        univ_mazs = maz_shp_landuse[maz_shp_landuse['tot_college_enroll'] > 0].reset_index(drop=True)\n",
    "        distances = [centroid.distance(geom) for geom in univ_mazs['geometry']]\n",
    "        row['closest_university_distance'] = np.amin(distances) / 5280 # ft to miles\n",
    "        row['closest_university_maz'] = univ_mazs.loc[np.argmin(distances), 'mgra']\n",
    "    return row\n",
    "    \n",
    "school_reassign = bad_school_shp.apply(lambda row: find_closest_valid_maz(row, maz_shp_landuse), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:46.838159Z",
     "start_time": "2020-11-12T20:56:46.635940Z"
    }
   },
   "outputs": [],
   "source": [
    "school_reassign['closest_gradeschool_distance'].hist(bins=20)\n",
    "plt.xlabel(\"Distance [miles]\")\n",
    "plt.title(\"Center of Invalid MAZ to closest valid MAZ\")\n",
    "plt.ylabel(\"Gradeschool Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.046892Z",
     "start_time": "2020-11-12T20:56:46.839662Z"
    }
   },
   "outputs": [],
   "source": [
    "school_reassign['closest_highschool_distance'].hist(bins=20)\n",
    "plt.xlabel(\"Distance [miles]\")\n",
    "plt.title(\"Center of Invalid MAZ to closest valid MAZ\")\n",
    "plt.ylabel(\"Highschool Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.107952Z",
     "start_time": "2020-11-12T20:56:47.048395Z"
    }
   },
   "outputs": [],
   "source": [
    "# univ_reassign_mazs = (school_reassign['closest_univ_maz'].notna())\n",
    "asim_per_with_school_df = pd.merge(asim_per_df, school_reassign, how='left', left_on='school_zone_id', right_on='MAZ')\n",
    "# asim_per_with_school_df.head()\n",
    "asim_per_with_school_df.loc[univ_students, 'school_zone_id'] = np.where(\n",
    "    asim_per_with_school_df.loc[univ_students, 'closest_university_maz'].isna(),\n",
    "    asim_per_with_school_df.loc[univ_students, 'school_zone_id'],\n",
    "    asim_per_with_school_df.loc[univ_students, 'closest_university_maz'])\n",
    "asim_per_with_school_df.loc[gradeschool_students, 'school_zone_id'] = np.where(\n",
    "    asim_per_with_school_df.loc[gradeschool_students, 'closest_gradeschool_maz'].isna(),\n",
    "    asim_per_with_school_df.loc[gradeschool_students, 'school_zone_id'],\n",
    "    asim_per_with_school_df.loc[gradeschool_students, 'closest_gradeschool_maz'])\n",
    "asim_per_with_school_df.loc[highschool_students, 'school_zone_id'] = np.where(\n",
    "    asim_per_with_school_df.loc[highschool_students, 'closest_highschool_maz'].isna(),\n",
    "    asim_per_with_school_df.loc[highschool_students, 'school_zone_id'],\n",
    "    asim_per_with_school_df.loc[highschool_students, 'closest_highschool_maz'])\n",
    "num_reassigned = (asim_per_with_school_df['school_zone_id'] != asim_per_df['school_zone_id']).sum()\n",
    "asim_per_df['school_zone_id'] = asim_per_with_school_df['school_zone_id']\n",
    "print(\"Number of invalid school MAZ's reassigned:\", num_reassigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.208060Z",
     "start_time": "2020-11-12T20:56:47.159019Z"
    }
   },
   "outputs": [],
   "source": [
    "new_school_co = make_school_co(asim_per_df)\n",
    "make_missing_school_maz_df(new_school_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_with_invalid_school_maz = asim_per_df.loc[asim_per_df['school_zone_id'] <= 0, 'person_id']\n",
    "asim_tour_df.loc[\n",
    "    (asim_tour_df['tour_type'] == 'school') \n",
    "    & asim_tour_df['person_id'].isin(people_with_invalid_school_maz),\n",
    "    'keep_tour'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If person makes a work tour, but has an invalid (missing) workplace maz, use the first work tour destination as workplace maz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_with_invalid_work_maz = asim_per_df.loc[asim_per_df['workplace_zone_id'] <= 0, 'person_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_workplace_mazs = asim_tour_df[(asim_tour_df['tour_type'] == 'work') & asim_tour_df['person_id'].isin(people_with_invalid_work_maz)][['destination', 'person_id']]\n",
    "inferred_workplace_mazs = inferred_workplace_mazs.drop_duplicates('person_id', keep='first')\n",
    "inferred_workplace_mazs.set_index('person_id', inplace=True)\n",
    "print(\"Inferred workplace maz for people that have a workplace zone_id missing but make a work tour: \")\n",
    "inferred_workplace_mazs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_per_df.set_index('person_id', inplace=True)\n",
    "asim_per_df.loc[inferred_workplace_mazs.index, 'workplace_zone_id'] = inferred_workplace_mazs['destination']\n",
    "asim_per_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinguishing between internal and external workers:\n",
    "asim_per_df['external_worker_identification'] = np.where(asim_per_df['workplace_zone_id'].isin(ext_gdf['MAZ']), 0, 1)\n",
    "asim_per_df['external_workplace_zone_id'] = np.where(asim_per_df['external_worker_identification'] == 0, asim_per_df['workplace_zone_id'], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change people that go to school and not work, but say they are workers\n",
    "Had a problem where an individual says they are a full time worker and a university student, but do not provide a work location and do not take a work tour, but do take a school tour. school_zone_id was changed to -1 after initialize households (full time workers do not get a school location), but cdap has 1 M school tour.  Since school_zone_id changed to -1, caused 0 probs error in mandatory tour frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_making_work_tours = asim_tour_df.loc[\n",
    "    (asim_tour_df['tour_type'] == 'work')\n",
    "    & (asim_tour_df['keep_tour'] == 1), 'person_id'].unique()\n",
    "people_making_school_tours = asim_tour_df.loc[\n",
    "    (asim_tour_df['tour_type'] == 'school') \n",
    "    & (asim_tour_df['keep_tour'] == 1), 'person_id'].unique()\n",
    "workers_who_are_actually_students = (asim_per_df['person_id'].isin(people_making_school_tours) \n",
    "                                    & (asim_per_df['ptype'] == 1) # ft worker\n",
    "                                    & ~asim_per_df['person_id'].isin(people_making_work_tours)\n",
    "                                    & (asim_per_df['pstudent'] < 3)  # school or university, not non-student\n",
    "                                    & (asim_per_df['school_zone_id'] > 0))\n",
    "# FIXME are we allowing ft workers to go to school?\n",
    "# if so, I think this is fine.  If not, need to change pytpe to part-time worker or univ students\n",
    "# asim_per_df.loc[workers_who_are_actually_students, 'ptype'] = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of workers who are actually students: \", workers_who_are_actually_students.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change people that go to work and not school, but say they are students\n",
    "Same problem as above -- peoply say they are students, but only go to work.  This makes for zero probs in mandatory tour frequency because they aren't assigned a workplace maz because they are not listed as workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_per_df.pemploy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_making_work_tours = asim_tour_df.loc[\n",
    "    (asim_tour_df['tour_type'] == 'work')\n",
    "    & (asim_tour_df['keep_tour'] == 1), 'person_id'].unique()\n",
    "people_making_school_tours = asim_tour_df.loc[\n",
    "    (asim_tour_df['tour_type'] == 'school') \n",
    "    & (asim_tour_df['keep_tour'] == 1), 'person_id'].unique()\n",
    "students_who_are_actually_workers = (asim_per_df['person_id'].isin(people_making_work_tours) \n",
    "                                    & (asim_per_df['pstudent'] < 3) # is a student\n",
    "                                    & ~asim_per_df['person_id'].isin(people_making_school_tours)\n",
    "                                    & (asim_per_df['school_zone_id'] < 0)\n",
    "                                    & (asim_per_df['workplace_zone_id'] > 0))\n",
    "\n",
    "asim_per_df.loc[students_who_are_actually_workers, 'ptype'] = 2 # part time worker\n",
    "asim_per_df.loc[students_who_are_actually_workers, 'pemploy'] = 2 # part time worker\n",
    "\n",
    "# # ptype and pstudent and pemploy are calculated on these fields\n",
    "# asim_per_df.loc[students_who_are_actually_workers, 'WKW'] = 1\n",
    "# asim_per_df.loc[students_who_are_actually_workers, 'WKHP'] = 40\n",
    "# asim_per_df.loc[students_who_are_actually_workers, 'SCHG'] = -9\n",
    "# asim_per_df.loc[students_who_are_actually_workers, 'ESR'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of students who are actually fulltime workers: \", students_who_are_actually_workers.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also want to count people who are making school and work tours but didn't list themselves as a worker. annotate_persons.csv determines pemploy and it just checks for ESR != [3,6] to determine partime status, so just need to ensure ESR = 1 for all people who make a work tour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting them to part time workers\n",
    "asim_per_df.loc[(asim_per_df['person_id'].isin(people_making_work_tours) \n",
    "                & (asim_per_df['workplace_zone_id'] > 0)), 'ptype'] = 2\n",
    "asim_per_df.loc[(asim_per_df['person_id'].isin(people_making_work_tours) \n",
    "                & (asim_per_df['workplace_zone_id'] > 0)), 'pemploy'] = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activitysim does not allow full-time workers to go to school.  Turning all full time workers going to school into part-time instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_time_workers_going_to_school = (asim_per_df['person_id'].isin(people_making_work_tours) \n",
    "                & asim_per_df['person_id'].isin(people_making_school_tours)\n",
    "                & (asim_per_df['pemploy'] == 1))\n",
    "# FIXME allow FT workers to go to school?\n",
    "# asim_per_df.loc[full_time_workers_going_to_school, 'WKW'] = 5  # 14-26 number of weeks worked\n",
    "# asim_per_df.loc[full_time_workers_going_to_school, 'WKHP'] = 35  # 35 hrs per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of full time workers also going to school: \", full_time_workers_going_to_school.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further person type checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_per_df['is_worker'] = asim_per_df['pemploy'].isin([1,2])  # full-time or part-time worker\n",
    "asim_per_df['is_student'] = asim_per_df['pstudent'].isin([1,2])  # grade/highschool or university"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (asim_per_df['is_worker'] & asim_per_df.workplace_zone_id.isna()).sum() == 0, \"not all workers have valid workplace zone id\"\n",
    "assert (asim_per_df['is_student'] & asim_per_df.school_zone_id.isna()).sum() == 0, \"not all students have valid school zone id\"\n",
    "assert (asim_per_df['is_worker'] & ~asim_per_df.ptype.isin([1,2,3,6])).sum() == 0, \"worker is not in the allowed person types\"\n",
    "assert (asim_per_df['is_student'] & ~asim_per_df.ptype.isin([1,2,3,6,7,8])).sum() == 0, \"student is not in the allowed person types\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove external starting and open jaw tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove external starting and open jaw tours\n",
    "count_before_tdd = len(asim_tour_df[asim_tour_df['keep_tour'] == 1])\n",
    "asim_tour_df.loc[~(asim_tour_df['external_type'].isin(['II', 'II-Ext', 'II-Ext(internal_dest)'])), 'keep_tour'] = 0\n",
    "# also removing external atwork subtours\n",
    "asim_tour_df.loc[~(asim_tour_df['external_type'].isin(['II']))&(asim_tour_df['tour_category']=='atwork'), 'keep_tour'] = 0\n",
    "\n",
    "count_after_tdd = len(asim_tour_df[asim_tour_df['keep_tour'] == 1])\n",
    "\n",
    "print(\"Removed an additional\", count_before_tdd - count_after_tdd, \"tours not starting and ending internally\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding external tour identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_tour_df['external_joint_tour_identification'] = np.where((asim_tour_df.external_type != \"II\") & (asim_tour_df.tour_category == 'joint'), 0, 1)\n",
    "asim_tour_df['external_non_mandatory_identification'] = np.where((asim_tour_df.external_type != \"II\") & (asim_tour_df.tour_category == 'non_mandatory'), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting school escorting variables\n",
    "Need to code escort type (ride_hail vs pure_escort) and chauffeur person ids here.  The rest is handled in infer.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chauf_type_dict = {\n",
    "    1: 'ride_share',\n",
    "    2: 'pure_escort',\n",
    "    3: pd.NA, # none\n",
    "}\n",
    "asim_tour_df['out_escort_type'] = asim_tour_df['OUT_ESCORT_TYPE'].map(chauf_type_dict)\n",
    "asim_tour_df['inb_escort_type'] = asim_tour_df['INB_ESCORT_TYPE'].map(chauf_type_dict)\n",
    "# asim_tour_df['out_escortee_purp'] = asim_tour_df['OUT_ESCORTEE_TOUR_PURP'].map(tour_purpose_spa_to_asim_dict)\n",
    "# asim_tour_df['inb_escortee_purp'] = asim_tour_df['INB_ESCORTEE_TOUR_PURP'].map(tour_purpose_spa_to_asim_dict)\n",
    "# FIXME Below flags are only for chauffeur tours, not escortee tours!  But we are instead looking at escortee trips in infer.py\n",
    "# asim_tour_df['school_esc_outbound'] = np.where(asim_tour_df['out_escortee_purp'] == 'school', asim_tour_df['out_escortee_type'], pd.NA)\n",
    "# asim_tour_df['school_esc_inbound'] = np.where(asim_tour_df['inb_escortee_purp'] == 'school', asim_tour_df['inb_escortee_type'], pd.NA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chauf_id_map = asim_per_df.set_index(['household_id', 'PER_ID'])['person_id'].to_dict()\n",
    "\n",
    "def map_chauf_id(row, col='OUT_CHAUFFUER_ID'):\n",
    "    if (row[col] == 'nan') | pd.isna(row[col]) | (row[col] == 'None'):\n",
    "        return pd.NA\n",
    "    try:\n",
    "        return chauf_id_map[(row['household_id'], int(row[col]))]\n",
    "    except ValueError:\n",
    "        print(row['household_id'], row[col])\n",
    "        return pd.NA\n",
    "\n",
    "\n",
    "asim_tour_df['out_chauf_person_id'] = asim_tour_df.apply(lambda row: map_chauf_id(row, 'OUT_CHAUFFUER_ID'), axis=1)\n",
    "asim_tour_df['inb_chauf_person_id'] = asim_tour_df.apply(lambda row: map_chauf_id(row, 'INB_CHAUFFUER_ID'), axis=1)\n",
    "assert len(asim_tour_df[~asim_tour_df['out_chauf_person_id'].isna()]) == len(asim_tour_df[(asim_tour_df['OUT_CHAUFFUER_ID'].fillna(-1).replace('None', -1).astype(int) > 0)]), \"Missing outbound chauffeur id\"\n",
    "assert len(asim_tour_df[~asim_tour_df['inb_chauf_person_id'].isna()]) == len(asim_tour_df[(asim_tour_df['INB_CHAUFFUER_ID'].fillna(-1).replace('None', -1).astype(int) > 0)]), \"Missing inbound chauffeur id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If person or parent tour is removed, also need to remove it's subtours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.230078Z",
     "start_time": "2020-11-12T20:56:47.220070Z"
    }
   },
   "outputs": [],
   "source": [
    "asim_tour_df.loc[\n",
    "      pd.notna(asim_tour_df['parent_tour_id'])\n",
    "      & ~(asim_tour_df['parent_tour_id'].isin(asim_tour_df.loc[asim_tour_df['keep_tour'] == 1, 'tour_id'])),\n",
    "    'keep_tour'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.248109Z",
     "start_time": "2020-11-12T20:56:47.231079Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total number of Tours: \", len(asim_tour_df))\n",
    "print(\"Total number of tours removed: \", len(asim_tour_df[asim_tour_df['keep_tour'] == 0]))\n",
    "trimmed_asim_tour_df = asim_tour_df[asim_tour_df['keep_tour'] == 1].copy()\n",
    "print(\"Final number of tours: \", len(trimmed_asim_tour_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also need to remove the tours from the joint tour participants file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_jtour_participants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.255124Z",
     "start_time": "2020-11-12T20:56:47.249613Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Initial number of joint tour participants: \", len(asim_jtour_participants_df))\n",
    "trimmed_asim_jtour_participants_df = asim_jtour_participants_df[asim_jtour_participants_df['tour_id'].isin(trimmed_asim_tour_df['tour_id'])]\n",
    "print(\"Final number of joint tour participants: \", len(trimmed_asim_jtour_participants_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not all joint tours have a matching entry in joint_tour_participants\n",
    "error found in infer.py 'assert (tour_has_adults | tour_has_children).all()' when trying to assign joint tour composition.  If tours are classified as joint, but there are no adults or children on the tour, an error is thrown.\n",
    "\n",
    "Only fully joint tours in the SPA tool have joint tour participants listed.\n",
    "\n",
    "In this script, setting the asim_tour_df[\"is_joint\"] = 1 for fully joint tours only solves this problem. The following code is a re-implementation of the code in infer.py to check that all joint tours have an adult or child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.261129Z",
     "start_time": "2020-11-12T20:56:47.256125Z"
    }
   },
   "outputs": [],
   "source": [
    "joint_tours = trimmed_asim_tour_df[trimmed_asim_tour_df['is_joint'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.288152Z",
     "start_time": "2020-11-12T20:56:47.262130Z"
    }
   },
   "outputs": [],
   "source": [
    "joint_tour_participants = pd.merge(\n",
    "    trimmed_asim_jtour_participants_df,\n",
    "    asim_per_df,\n",
    "    how='left',\n",
    "    on='person_id'\n",
    ")\n",
    "joint_tour_participants['adult'] = (joint_tour_participants.age >= 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.301163Z",
     "start_time": "2020-11-12T20:56:47.289153Z"
    }
   },
   "outputs": [],
   "source": [
    "tour_has_adults = joint_tour_participants[joint_tour_participants.adult]\\\n",
    "        .groupby('tour_id').size().reindex(joint_tours['tour_id']).fillna(0) > 0\n",
    "tour_has_adults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.310170Z",
     "start_time": "2020-11-12T20:56:47.302164Z"
    }
   },
   "outputs": [],
   "source": [
    "tour_has_children = joint_tour_participants[~joint_tour_participants.adult]\\\n",
    "        .groupby('tour_id').size().reindex(joint_tours['tour_id']).fillna(0) > 0\n",
    "tour_has_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.314174Z",
     "start_time": "2020-11-12T20:56:47.311171Z"
    }
   },
   "outputs": [],
   "source": [
    "tour_has_children.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.319178Z",
     "start_time": "2020-11-12T20:56:47.315175Z"
    }
   },
   "outputs": [],
   "source": [
    "tour_has_adults.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:56:47.328185Z",
     "start_time": "2020-11-12T20:56:47.325183Z"
    }
   },
   "outputs": [],
   "source": [
    "good_tours = (tour_has_children | tour_has_adults)\n",
    "good_tours.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert good_tours.sum() == len(joint_tours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trips Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_out_trips_df = pd.merge(\n",
    "    spa_out_trips_df,\n",
    "    asim_tour_df[['HH_ID', 'PER_ID', 'TOUR_ID', 'household_id', 'person_id', 'tour_id', 'day', 'tour_purpose', 'external_type']],\n",
    "    on=['HH_ID', 'PER_ID', 'TOUR_ID', 'day'],\n",
    "    how='left',\n",
    "    suffixes=('', '_x')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_out_trips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asim_trip_df = pd.DataFrame()\n",
    "\n",
    "keep_cols = ['household_id', 'person_id', 'tour_id', 'day', 'survey_year', 'HH_ID', \n",
    "'PER_ID', 'TOUR_ID', 'TRIP_ID', 'ORIG_X', 'ORIG_Y', 'DEST_X', 'DEST_Y', 'external_type',\n",
    "'ESCORTED', 'ESCORTING', 'NUM_PERSONS_ESCORTED', 'ESCORT_PERS_1', 'ESCORT_PERS_2', \n",
    "'ESCORT_PERS_3', 'ESCORT_PERS_4', 'ESCORT_PERS_5', 'DEST_ESCORTING']\n",
    "\n",
    "asim_trip_df[keep_cols] = spa_out_trips_df[keep_cols]\n",
    "asim_trip_df['origin'] = spa_out_trips_df['ORIG_MAZ']\n",
    "asim_trip_df['destination'] = spa_out_trips_df['DEST_MAZ']\n",
    "asim_trip_df['depart'] = spa_out_trips_df['ORIG_DEP_BIN']\n",
    "asim_trip_df['primary_purpose'] = spa_out_trips_df['tour_purpose']\n",
    "asim_trip_df['purpose'] = spa_out_trips_df['DEST_PURP'].map(tour_purpose_spa_to_asim_dict)\n",
    "# asim_trip_df['trip_mode'] = spa_out_trips_df['TRIPMODE'].map(tour_mode_spa_to_asim_dict)\n",
    "asim_trip_df['trip_num'] = spa_out_trips_df['TRIP_ID']\n",
    "asim_trip_df['outbound'] = np.where(spa_out_trips_df['IS_INBOUND'] == 0, True, False)\n",
    "asim_trip_df['home_zone_id'] = reindex(asim_hh_df.set_index('household_id')['home_zone_id'], asim_trip_df.household_id)\n",
    "asim_trip_df['workplace_zone_id'] = reindex(asim_per_df.set_index('person_id')['workplace_zone_id'], asim_trip_df.person_id)\n",
    "asim_trip_df['school_zone_id'] = reindex(asim_per_df.set_index('person_id')['workplace_zone_id'], asim_trip_df.person_id)\n",
    "asim_trip_df['is_subtour'] = reindex(asim_tour_df.set_index('tour_id')['is_subtour'], asim_trip_df.tour_id)\n",
    "asim_trip_df['parent_tour_id'] = reindex(asim_tour_df.set_index('tour_id')['parent_tour_id'], asim_trip_df.tour_id)\n",
    "asim_trip_df['tour_category'] = reindex(asim_tour_df.set_index('tour_id')['tour_category'], asim_trip_df.tour_id)\n",
    "# calculating trip_num and trip_count\n",
    "grouped = asim_trip_df.groupby([\"tour_id\", \"outbound\"])\n",
    "asim_trip_df[\"trip_num\"] = grouped.cumcount() + 1\n",
    "asim_trip_df[\"trip_count\"] = asim_trip_df[\"trip_num\"] + grouped.cumcount(ascending=False)\n",
    "asim_trip_df['trip_id'] = asim_trip_df.reset_index().index + 1\n",
    "asim_trip_df['keep_trip'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022 survey has same trip & tour modes\n",
    "asim_trip_df.loc[asim_trip_df['survey_year'] == 2022, 'trip_mode'] = spa_out_trips_df.loc[\n",
    "    spa_out_trips_df['survey_year'] == 2022, 'TRIPMODE'].map(tour_mode_spa_to_asim_dict_22)\n",
    "\n",
    "# 2016 survey was re-processed to 2022 survey modes\n",
    "asim_trip_df.loc[asim_trip_df['survey_year'] == 2016, 'trip_mode'] = spa_out_trips_df.loc[\n",
    "    spa_out_trips_df['survey_year'] == 2016, 'TRIPMODE'].map(tour_mode_spa_to_asim_dict_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform additional checks on the survey data.\n",
    " 1. Do all trips have a tour?\n",
    " 2. Are there four or fewer trips per tour and direction?\n",
    " 3. Do all tours have a trip?\n",
    " 4. Do all trips have a person and household?\n",
    " 5. Do all tours (except subtours) start and end at home?\n",
    " 6. Do mandatory (work and school) tours end at same zone as person work/school zone ids?\n",
    " 7. Do all tours have persons and households?\n",
    " 8. Do all persons have a household?\n",
    " 9. Do trip purposes match destination_choice_size_terms \n",
    "10. Do the joint tours match (tour, person, hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Do all trips still belong to a valid tour?\n",
    "count_before_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "asim_trip_df.loc[~asim_trip_df['tour_id'].isin(trimmed_asim_tour_df.tour_id), 'keep_trip'] = 0\n",
    "\n",
    "count_after_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "print(\"Removed\", count_before_removed_trips - count_after_removed_trips, \"trips because their tour was removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Do all trips go to a valid zone?\n",
    "count_before_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "asim_trip_df.loc[(asim_trip_df['origin'] <= 0) | (asim_trip_df['destination'] <= 0), 'keep_trip'] = 0\n",
    "\n",
    "count_after_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "print(\"Removed\", count_before_removed_trips - count_after_removed_trips, \"trips because of bad origin / destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Are there four or fewer trips per tour and direction?\n",
    "count_before_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "# do not remove primary tour destination trip or home trip\n",
    "unallowed_trips = (\n",
    "    (asim_trip_df['trip_num'] > 3)\n",
    "    & (asim_trip_df['trip_num'] != asim_trip_df['trip_count'])\n",
    ")\n",
    "\n",
    "asim_trip_df.loc[unallowed_trips, 'keep_trip'] = 0\n",
    "\n",
    "count_after_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "print(\"Removed an additional\", count_before_removed_trips - count_after_removed_trips, \"trips because their trip number is > 4\")\n",
    "print(f\"Happens on {len(asim_trip_df.loc[unallowed_trips, 'tour_id'].unique())} tours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Do all trips have a person & household?\n",
    "count_before_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "asim_trip_df.loc[~asim_trip_df['household_id'].isin(asim_hh_df.household_id) | ~asim_trip_df['person_id'].isin(asim_per_df.person_id), 'keep_trip'] = 0\n",
    "\n",
    "count_after_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "print(\"Removed an additional\", count_before_removed_trips - count_after_removed_trips, \"trips did not belong to a person or hh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActivitySim only allows subtours on work tours -- removing all other subtours\n",
    "count_before_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "# should remove the whole tour since these are tour-level variables\n",
    "unallowed_trips = (\n",
    "    (asim_trip_df['is_subtour'] == 1)\n",
    "    & (asim_trip_df['tour_category'] != 'atwork')\n",
    ")\n",
    "\n",
    "asim_trip_df.loc[unallowed_trips, 'keep_trip'] = 0\n",
    "\n",
    "count_after_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "print(\"Removed an additional\", count_before_removed_trips - count_after_removed_trips, \"trips because they are on non-atwork subtours\")\n",
    "print(f\"Happens on {len(asim_trip_df.loc[unallowed_trips, 'tour_id'].unique())} tours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore tours not starting at home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_hh_16['year'] = 2016\n",
    "raw_hh_22['year'] = 2022\n",
    "\n",
    "def get_xy_in_feet(df, lat_col='home_lat', lon_col='home_lng', prefix='home'):\n",
    "    gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(x=df[lon_col], y=df[lat_col]))\n",
    "    # WGS84 to CA Stateplane \n",
    "    gdf.set_crs(epsg=4326, inplace=True, allow_override=True)\n",
    "    gdf.to_crs(epsg=2230, inplace=True)\n",
    "    df[prefix + '_x'] = gdf.geometry.x\n",
    "    df[prefix + '_y'] = gdf.geometry.y\n",
    "    return df\n",
    "\n",
    "raw_hh_16 = get_xy_in_feet(raw_hh_16, lat_col='home_lat', lon_col='home_lng', prefix='home')\n",
    "raw_hh_22 = get_xy_in_feet(raw_hh_22, lat_col='home_lat', lon_col='home_lon', prefix='home')\n",
    "raw_hh = pd.concat([raw_hh_16[['hhid', 'year', 'home_x', 'home_y']].rename(columns={'hhid': 'hh_id'}), \n",
    "                    raw_hh_22[['hh_id', 'year', 'home_x', 'home_y']]])\n",
    "raw_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to calculate distance between two points\n",
    "def calc_dist(x1, y1, x2, y2):\n",
    "    distance = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the mismatching \n",
    "# trips not starting at home\n",
    "bad_starts = asim_trip_df[(asim_trip_df['trip_num'] == 1) & \n",
    "                          (asim_trip_df['outbound']) & \n",
    "                          (asim_trip_df['is_subtour'] == 0) & \n",
    "                          (asim_trip_df['home_zone_id'] != asim_trip_df['origin'])].copy()\n",
    "\n",
    "# converting X and Y starts to California state plane CRS\n",
    "bad_starts = get_xy_in_feet(bad_starts, lat_col='ORIG_Y', lon_col='ORIG_X', prefix='origin')\n",
    "\n",
    "# merging in home x & y\n",
    "bad_starts = pd.merge(bad_starts, \n",
    "                      raw_hh[['hh_id', 'year', 'home_x', 'home_y']], \n",
    "                      left_on=['HH_ID', 'survey_year'],\n",
    "                      right_on=['hh_id', 'year'],\n",
    "                      how='left',\n",
    "                      suffixes=('', '_x'))\n",
    "\n",
    "bad_starts['origin_home_dist'] = bad_starts.apply(lambda row: calc_dist(x1=row.origin_x, \n",
    "                                                                        y1=row.origin_y,\n",
    "                                                                        x2=row.home_x,\n",
    "                                                                        y2=row.home_y), axis=1)\n",
    "\n",
    "bad_starts['acceptable_origin_home_dist'] = 0\n",
    "bad_starts.loc[bad_starts['origin_home_dist']<=550, 'acceptable_origin_home_dist'] = 1\n",
    "\n",
    "print(f\"{len(bad_starts)} trips do not start at home, but {bad_starts['acceptable_origin_home_dist'].sum()} are within 550 feet\")\n",
    "\n",
    "### Merge the acceptable dist column to asim trips file\n",
    "asim_trip_df.loc[bad_starts.index, 'acceptable_origin_home_dist'] = bad_starts['acceptable_origin_home_dist']\n",
    "asim_trip_df['acceptable_origin_home_dist'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Does first trip on tour start at home (or) the difference b/w mismatching MAZ is < 550 ft\n",
    "count_before_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "bad_starts = asim_trip_df[(asim_trip_df['trip_num'] == 1) & \n",
    "                          (asim_trip_df['outbound']) & \n",
    "                          (asim_trip_df['is_subtour'] == 0) & \n",
    "                          (asim_trip_df['home_zone_id'] != asim_trip_df['origin']) & \n",
    "                          (asim_trip_df['acceptable_origin_home_dist'] != 1)]\n",
    "\n",
    "asim_trip_df.loc[asim_trip_df['tour_id'].isin(bad_starts.tour_id), 'keep_trip'] = 0\n",
    "# need to also change origin to home zone if within threshold\n",
    "asim_trip_df.loc[asim_trip_df['acceptable_origin_home_dist'] == 1, 'origin'] = asim_trip_df.loc[asim_trip_df['acceptable_origin_home_dist'] == 1, 'home_zone_id']\n",
    "\n",
    "count_after_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "print(\"Removed an additional\", count_before_removed_trips - count_after_removed_trips, \"trips that belong to tours that do not start at home\")\n",
    "print(f\"Happens for {len(bad_starts)} tours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring tours not ending at home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ends = asim_trip_df[\n",
    "    (asim_trip_df['trip_num'] == asim_trip_df['trip_count']) \n",
    "    & (~asim_trip_df['outbound']) & (asim_trip_df['is_subtour'] == 0) \n",
    "    & ((asim_trip_df['home_zone_id'] != asim_trip_df['destination']) | (asim_trip_df['purpose'] != 'home'))\n",
    "    ].copy()\n",
    "\n",
    "# converting X and Y starts to California state plane CRS\n",
    "bad_ends = get_xy_in_feet(bad_ends, lat_col='DEST_Y', lon_col='DEST_X', prefix='destination')\n",
    "\n",
    "# merging home x & y\n",
    "bad_ends = pd.merge(bad_ends, \n",
    "                    raw_hh[['hh_id', 'year', 'home_x', 'home_y']], \n",
    "                    left_on=['HH_ID', 'survey_year'],\n",
    "                    right_on=['hh_id', 'year'],\n",
    "                    how='left', \n",
    "                    suffixes=('', '_x'))\n",
    "\n",
    "bad_ends['dest_home_dist'] = bad_ends.apply(lambda row: calc_dist(x1=row.destination_x, \n",
    "                                                                  y1=row.destination_y,\n",
    "                                                                  x2=row.home_x,\n",
    "                                                                  y2=row.home_y), axis=1)\n",
    "\n",
    "bad_ends['acceptable_dest_home_dist'] = 0\n",
    "bad_ends.loc[bad_ends['dest_home_dist']<=550, 'acceptable_dest_home_dist'] = 1\n",
    "\n",
    "print(f\"{len(bad_starts)} trips do not end at home, but {bad_starts['acceptable_origin_home_dist'].sum()} are within 550 feet\")\n",
    "\n",
    "### Merge the acceptable dist column to asim trips file\n",
    "asim_trip_df.loc[bad_ends.index, 'acceptable_dest_home_dist'] = bad_ends['acceptable_dest_home_dist']\n",
    "asim_trip_df['acceptable_dest_home_dist'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Does last trip on tour end at home (or) the difference b/w mismatching MAZ is < 550 ft\n",
    "count_before_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "bad_ends = asim_trip_df[\n",
    "    (asim_trip_df['trip_num'] == asim_trip_df['trip_count']) \n",
    "    & (~asim_trip_df['outbound']) & (asim_trip_df['is_subtour'] == 0) \n",
    "    & ((asim_trip_df['home_zone_id'] != asim_trip_df['destination']) | (asim_trip_df['purpose'] != 'home'))\n",
    "    & (asim_trip_df['acceptable_dest_home_dist'] != 1)\n",
    "    ]\n",
    "\n",
    "asim_trip_df.loc[asim_trip_df['tour_id'].isin(bad_ends.tour_id), 'keep_trip'] = 0\n",
    "# need to also change destination to home zone if within threshold\n",
    "asim_trip_df.loc[asim_trip_df['acceptable_dest_home_dist'] == 1, 'destination'] = asim_trip_df.loc[asim_trip_df['acceptable_dest_home_dist'] == 1, 'home_zone_id']\n",
    "count_after_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "print(\"Removed an additional\", count_before_removed_trips - count_after_removed_trips, \"trips that belong to tours that do not end at home\")\n",
    "print(f\"Happens for {len(bad_ends)} tours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do school and work tours go to the school or workplace zone?\n",
    "# just changing primary destination instead of removing them.  Adding flag to filter if needed\n",
    "asim_trip_df['mand_dest_changed'] = 0\n",
    "\n",
    "bad_school_trips = (\n",
    "    (asim_trip_df['purpose'] == 'school') \n",
    "    & (asim_trip_df['destination'] != asim_trip_df['school_zone_id'])\n",
    "    & (asim_trip_df['school_zone_id'] > 0)\n",
    ")\n",
    "asim_trip_df.loc[bad_school_trips, 'destination'] = asim_trip_df.loc[bad_school_trips, 'school_zone_id']\n",
    "asim_trip_df.loc[bad_school_trips, 'mand_dest_changed'] = 1\n",
    "print(f\"School trip does not go to school zone for {bad_school_trips.sum()} trips\")\n",
    "\n",
    "bad_work_trips = (\n",
    "    (asim_trip_df['purpose'] == 'work') \n",
    "    & (asim_trip_df['destination'] != asim_trip_df['workplace_zone_id'])\n",
    "    & (asim_trip_df['workplace_zone_id'] > 0)\n",
    ")\n",
    "asim_trip_df.loc[bad_work_trips, 'destination'] = asim_trip_df.loc[bad_work_trips, 'school_zone_id']\n",
    "asim_trip_df.loc[bad_work_trips, 'mand_dest_changed'] = 1\n",
    "print(f\"Work trip does not go to workplace zone for {bad_work_trips.sum()} trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Atwork subtours need to start at the workplace location\n",
    "\n",
    "bad_atwork_origins = (\n",
    "    (asim_trip_df['is_subtour'] == 1)\n",
    "    & (asim_trip_df['tour_category'] == 'atwork')\n",
    "    & (asim_trip_df['trip_num'] == 1)\n",
    "    & (asim_trip_df['outbound'] == True)\n",
    "    & (asim_trip_df['origin'] != asim_trip_df['workplace_zone_id'])\n",
    ")\n",
    "\n",
    "asim_trip_df.loc[bad_atwork_origins, 'origin'] = asim_trip_df.loc[bad_atwork_origins, 'workplace_zone_id']\n",
    "\n",
    "print(\"Changed origins for \", bad_atwork_origins.sum(), \"trips on atwork subtours who didn't start at work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Are there at least two trips in the tour?\n",
    "count_before_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "trip_counts = asim_trip_df.groupby(['tour_id'])['keep_trip'].sum()\n",
    "tours_with_lt_2_trips = trip_counts[trip_counts < 2].index\n",
    "\n",
    "asim_trip_df.loc[asim_trip_df['tour_id'].isin(tours_with_lt_2_trips), 'keep_trip'] = 0\n",
    "\n",
    "count_after_removed_trips = len(asim_trip_df[asim_trip_df['keep_trip'] == 1])\n",
    "\n",
    "print(\"Removed an additional\", count_before_removed_trips - count_after_removed_trips, \"trips because they belonged to tours that didn't have at least 2 trips\")\n",
    "print(f\"Happens for {len(tours_with_lt_2_trips.unique())} tours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Number of trips kept after filtering:\")\n",
    "asim_trip_df['keep_trip'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of trip that are kept after filtering:\")\n",
    "asim_trip_df['keep_trip'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding all trips to/from external TAZ 11 to external TAZ 12:\n",
    "Extenal TAZ 11 is a dummy external station in the northwest corner of the region and has no counts in the intenal-external counts file and therefore no size-term in ActivitySim.  External TAZ 12 is the north I-5 external station.  Because TAZ 11 is actually closer to LA than TAZ 11, almost all external trips/tours are being geo-coded to TAZ 11 instead of TAZ 12.  Adding logic here to change any origin or destination that has the dummy TAZ 11 to the I-5 external station TAZ 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_11_maz = landuse.loc[landuse.taz == 11, 'MAZ'].values[0]\n",
    "taz_12_maz = landuse.loc[landuse.taz == 12, 'MAZ'].values[0]\n",
    "\n",
    "num_trips_to_taz_11 = ((asim_trip_df.origin == taz_11_maz) | (asim_trip_df.destination == taz_11_maz)).sum()\n",
    "num_tours_to_taz_11 = ((asim_tour_df.origin == taz_11_maz) | (asim_tour_df.destination == taz_11_maz)).sum()\n",
    "print(\"number of tours to or from taz 11: \", num_tours_to_taz_11)\n",
    "print(\"number of trips to or from taz 11: \", num_trips_to_taz_11)\n",
    "\n",
    "asim_trip_df.loc[asim_trip_df.origin == taz_11_maz, 'origin'] = taz_12_maz\n",
    "asim_trip_df.loc[asim_trip_df.destination == taz_11_maz, 'destination'] = taz_12_maz\n",
    "asim_tour_df.loc[asim_tour_df.origin == taz_11_maz, 'origin'] = taz_12_maz\n",
    "asim_tour_df.loc[asim_tour_df.destination == taz_11_maz, 'destination'] = taz_12_maz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to check tour table for consistency again\n",
    " Do all tours have trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_asim_trip_df = asim_trip_df[asim_trip_df['keep_trip'] == 1]\n",
    "\n",
    "print(f\"Tours before trip cleaning: {len(trimmed_asim_tour_df)}\")\n",
    "# removing tours that have trips removed\n",
    "trimmed_asim_tour_df = trimmed_asim_tour_df[trimmed_asim_tour_df['tour_id'].isin(trimmed_asim_trip_df.tour_id)]\n",
    "# removing subtours if their parent tour was removed\n",
    "trimmed_asim_tour_df = trimmed_asim_tour_df[trimmed_asim_tour_df['parent_tour_id'].isin(trimmed_asim_trip_df.tour_id) | (trimmed_asim_tour_df['is_subtour'] == 0)]\n",
    "print(f\"Tours after trip cleaning: {len(trimmed_asim_tour_df)}\")\n",
    "\n",
    "print(\"Joint tour participants before trip cleaning: \", len(trimmed_asim_jtour_participants_df))\n",
    "trimmed_asim_jtour_participants_df = trimmed_asim_jtour_participants_df[trimmed_asim_jtour_participants_df['tour_id'].isin(trimmed_asim_trip_df['tour_id'])]\n",
    "print(\"Joint tour participants after trip cleaning: \", len(trimmed_asim_jtour_participants_df))\n",
    "\n",
    "print(f\"Trips before tour re-cleaning: {len(trimmed_asim_trip_df)}\")\n",
    "trimmed_asim_trip_df = trimmed_asim_trip_df[trimmed_asim_trip_df['tour_id'].isin(trimmed_asim_tour_df.tour_id)]\n",
    "print(f\"Trips after tour re-cleaning: {len(trimmed_asim_trip_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-calculating trip variables after trimming:\n",
    "\n",
    "# origins and destinations need to be consistent.\n",
    "# in previous trip cleaning procedures, origins were kept consistent when filtering out extra stops\n",
    "# but the destinations were changed for trips to mandatory tour locations\n",
    "# sooo, resetting origin when the previous trip was mandatory and then making all destinations consistent with origin\n",
    "trimmed_asim_trip_df['prev_trip_purpose'] = trimmed_asim_trip_df.groupby('tour_id')['purpose'].shift(1)\n",
    "trimmed_asim_trip_df['prev_trip_dest'] = trimmed_asim_trip_df.groupby('tour_id')['destination'].shift(1)\n",
    "prev_mand_trips = trimmed_asim_trip_df['prev_trip_purpose'].isin(['work', 'school', 'univ'])\n",
    "trimmed_asim_trip_df.loc[prev_mand_trips, 'origin'] = trimmed_asim_trip_df.loc[prev_mand_trips, 'prev_trip_dest']\n",
    "\n",
    "# destination is the next trips origin\n",
    "new_dest = trimmed_asim_trip_df.groupby('tour_id')['origin'].shift(-1)\n",
    "new_dest = new_dest[~new_dest.isna()] # ignoring trips to home\n",
    "\n",
    "# re-numbering and counting trips\n",
    "grouped = trimmed_asim_trip_df.groupby([\"tour_id\", \"outbound\"])\n",
    "trimmed_asim_trip_df[\"trip_num\"] = grouped.cumcount() + 1\n",
    "trimmed_asim_trip_df[\"trip_count\"] = trimmed_asim_trip_df[\"trip_num\"] + grouped.cumcount(ascending=False)\n",
    "trimmed_asim_trip_df['trip_id'] = trimmed_asim_trip_df.reset_index().index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_asim_trip_df[trimmed_asim_trip_df.tour_id == 4727]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_asim_trip_df[trimmed_asim_trip_df.tour_id == 4725]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Additional Required Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: making these up right now!\n",
    "asim_per_df['educ'] = np.where(asim_per_df.age >= 18, 9, 0)\n",
    "asim_per_df['educ'] = np.where(asim_per_df.age >= 22, 13, asim_per_df.educ)\n",
    "# asim_hh_df['school_escorting_outbound'] = 1  # no escorting\n",
    "# asim_hh_df['school_escorting_inbound'] = 1  # no escorting\n",
    "# asim_hh_df['school_escorting_outbound_cond'] = asim_hh_df['school_escorting_outbound']  # only one real decision\n",
    "# asim_hh_df['bldgsz'] = np.where(asim_hh_df.HHT.isin([1,2,3]), 2, -1) # detacted single family home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Output and Running Infer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_output_cols = [\n",
    "    'household_id',\n",
    "    'home_zone_id',\n",
    "    'income',\n",
    "    'hhsize',\n",
    "    'HHT',\n",
    "    'auto_ownership',\n",
    "    'num_workers',\n",
    "    'children',\n",
    "    'day',\n",
    "    'bldgsz',\n",
    "    'res_type',\n",
    "    'transponder_ownership',\n",
    "    'survey_year',\n",
    "    'day',\n",
    "    'HH_ID',\n",
    "]\n",
    "output_asim_hh_df = asim_hh_df[hh_output_cols].copy()\n",
    "per_output_cols = [\n",
    "    'person_id',\n",
    "    'household_id',\n",
    "    'PNUM',\n",
    "    'age',\n",
    "    'sex',\n",
    "    'pemploy',\n",
    "    'pstudent',\n",
    "    'is_student',\n",
    "    'ptype',\n",
    "    'school_zone_id',\n",
    "    'workplace_zone_id',\n",
    "    'free_parking_at_work',\n",
    "    'work_from_home',\n",
    "    'telecommute_frequency',\n",
    "    'day',\n",
    "    'educ',\n",
    "    'external_worker_identification',\n",
    "    'external_workplace_zone_id',\n",
    "    'transit_pass_subsidy',\n",
    "    'transit_pass_ownership',\n",
    "    'industry',\n",
    "]\n",
    "output_asim_per_df = asim_per_df[per_output_cols].copy()\n",
    "tour_output_cols = [\n",
    "    'tour_id',\n",
    "    'person_id',\n",
    "    'household_id',\n",
    "    'tour_type',\n",
    "    'tour_category',\n",
    "    'origin',\n",
    "    'destination',\n",
    "    'start',\n",
    "    'end',\n",
    "    'tour_mode',\n",
    "    'parent_tour_id',\n",
    "    'day',\n",
    "    'external_joint_tour_identification',\n",
    "    'external_non_mandatory_identification',\n",
    "    'out_escort_type', # escort type of the person being escorted\n",
    "    'inb_escort_type',\n",
    "    'out_chauf_person_id',\n",
    "    'inb_chauf_person_id',\n",
    "]\n",
    "output_asim_tour_df = trimmed_asim_tour_df[tour_output_cols].copy()\n",
    "jtour_output_cols = [\n",
    "    'participant_id',\n",
    "    'tour_id',\n",
    "    'household_id',\n",
    "    'person_id',\n",
    "    'participant_num',\n",
    "    'day'\n",
    "]\n",
    "output_asim_jtour_df = trimmed_asim_jtour_participants_df[jtour_output_cols].copy()\n",
    "\n",
    "trip_output_cols = [\n",
    "    'household_id',\n",
    "    'tour_id',\n",
    "    'person_id',\n",
    "    'trip_id',\n",
    "    'day',\n",
    "    'origin',\n",
    "    'destination',\n",
    "    'depart',\n",
    "    'primary_purpose',\n",
    "    'purpose',\n",
    "    'trip_mode',\n",
    "    'trip_num',\n",
    "    'outbound',\n",
    "    'trip_count'\n",
    "]\n",
    "output_asim_trip_df = trimmed_asim_trip_df[trip_output_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_asim_hh_df.to_csv(os.path.join(final_output_path, \"survey_households.csv\"), index=False)\n",
    "output_asim_per_df.to_csv(os.path.join(final_output_path, \"survey_persons.csv\"), index=False)\n",
    "output_asim_tour_df.to_csv(os.path.join(final_output_path, \"survey_tours.csv\"), index=False)\n",
    "output_asim_jtour_df.to_csv(os.path.join(final_output_path, \"survey_joint_tour_participants.csv\"), index=False)\n",
    "output_asim_trip_df.to_csv(os.path.join(final_output_path, \"survey_trips.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "os.chdir(estimation_path)\n",
    "infer_py_location = r\"{dir}\\scripts\\infer.py\".format(dir=estimation_path)\n",
    "infer_run_command = \"python \" + infer_py_location + \" \" + estimation_path + \" \" + configs_dir\n",
    "infer_result = os.system(infer_run_command)\n",
    "if infer_result == 0:\n",
    "    print(\"infer script successfully completed\")\n",
    "else:\n",
    "    print(\"Error in infer script!\")\n",
    "os.chdir(cur_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating External Targets Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a tour leaves and enters the region through multiple external stations, assign it the most crossed station\n",
    "# If they are equal, it takes the first instance\n",
    "\n",
    "### Identifying most frequented external station for non-mandatory (or) joint II-Ext(internal_dest) tours\n",
    "dest_change_tour_df = asim_tour_df[(asim_tour_df['keep_tour']==1)&\n",
    "                                   (asim_tour_df['external_type']=='II-Ext(internal_dest)')&\n",
    "                                   (asim_tour_df['tour_category'].isin(['non_mandatory', 'joint']))]\n",
    "\n",
    "dest_change_trip_df = asim_trip_df[asim_trip_df['tour_id'].isin(dest_change_tour_df['tour_id'])&\n",
    "                                   asim_trip_df['destination'].isin(ext_gdf['MAZ'])]\n",
    "\n",
    "most_freq_dest_df = dest_change_trip_df.groupby(['tour_id'])['destination'].agg(lambda x: pd.Series.mode(x)[0]).to_frame()\n",
    "\n",
    "### Updating the destination for non-mandatory II-Ext(internal_dest) tours\n",
    "asim_tour_df.set_index('tour_id', inplace=True)\n",
    "asim_tour_df.loc[most_freq_dest_df.index, 'destination'] = most_freq_dest_df['destination']\n",
    "asim_tour_df.reset_index(inplace=True)\n",
    "\n",
    "### Summary table of external type and tour category\n",
    "asim_tour_df[asim_tour_df['keep_tour']==1].groupby(['external_type', 'tour_category']).size().reset_index().rename(columns={0: 'count'})\n",
    "## Creating targets for external station\n",
    "### Filter out invalid tours and tours that are fully internal or starting externally\n",
    "target_tours_df = asim_tour_df[(asim_tour_df['keep_tour']==1)&(~asim_tour_df['external_type'].isin(['II']))].copy()\n",
    "\n",
    "### Create a summary table counting tours at each external station by tour_category\n",
    "target_tours_df['tour_category'].replace({'joint': 'non_mandatory'}, inplace=True)\n",
    "external_targets = pd.pivot_table(target_tours_df, \n",
    "                                  columns='tour_category', \n",
    "                                  index='destination', \n",
    "                                  values='tour_id', \n",
    "                                  aggfunc='count', \n",
    "                                  margins=True, \n",
    "                                  margins_name='Total', \n",
    "                                  fill_value=0).reset_index().rename(columns={'destination': 'external_station'})\n",
    "\n",
    "### Add TAZ number and sort the targets by it\n",
    "external_targets['external_station'] = external_targets['external_station'].replace(ext_gdf.set_index('MAZ')['TAZ'])\n",
    "external_targets = external_targets[external_targets['external_station'].isin(ext_gdf['TAZ'])]\n",
    "external_targets.sort_values('external_station', inplace=True)\n",
    "\n",
    "external_targets.loc[len(external_targets.index)] = ['Total', \n",
    "                                                     external_targets['mandatory'].sum(), \n",
    "                                                     external_targets['non_mandatory'].sum(), \n",
    "                                                     external_targets['Total'].sum()]\n",
    "\n",
    "external_targets.set_index('external_station', inplace=True)\n",
    "external_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external_targets.to_csv(os.path.join(final_output_path, \"external_station_targets.csv\"), index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to make cut and create set of zones to test estimation mode in a cropped zone system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_households = pd.read_csv(os.path.join(final_output_path, \"override_households.csv\"))\n",
    "or_persons = pd.read_csv(os.path.join(final_output_path, \"override_persons.csv\"))\n",
    "or_tours = pd.read_csv(os.path.join(final_output_path, \"override_tours.csv\"))\n",
    "or_jtp = pd.read_csv(os.path.join(final_output_path, \"override_joint_tour_participants.csv\"))\n",
    "or_trips = pd.read_csv(os.path.join(final_output_path, \"override_trips.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_relevant_zones(household_ids, crop_output_path=None):\n",
    "    zones = []\n",
    "    cut_households = or_households[or_households.household_id.isin(household_ids)]\n",
    "    # home zone id's\n",
    "    zones.append(cut_households.home_zone_id.unique())\n",
    "\n",
    "    # person school and work locations\n",
    "    cut_persons = or_persons[or_persons.household_id.isin(household_ids)]\n",
    "    zones.append(cut_persons.school_zone_id.unique())\n",
    "    zones.append(cut_persons.workplace_zone_id.unique())\n",
    "\n",
    "    # tour origins and destinations\n",
    "    cut_tours = or_tours[or_tours.household_id.isin(household_ids)]\n",
    "    zones.append(cut_tours.origin.unique())\n",
    "    zones.append(cut_tours.destination.unique())\n",
    "\n",
    "    # trip orogins and destinations\n",
    "    cut_trips = or_trips[or_trips.household_id.isin(household_ids)]\n",
    "    zones.append(cut_trips.origin.unique())\n",
    "    zones.append(cut_trips.destination.unique())\n",
    "\n",
    "    zones = np.unique(np.concatenate(zones))\n",
    "    zones = zones[zones >= 0].astype(int)\n",
    "    print(f\"Set of households has {len(zones)} relevant zones.\")\n",
    "\n",
    "    if crop_output_path:\n",
    "        cut_jtp = or_jtp[or_jtp.household_id.isin(household_ids)]\n",
    "\n",
    "        cut_households.to_csv(os.path.join(crop_output_path, 'override_households.csv'))\n",
    "        cut_persons.to_csv(os.path.join(crop_output_path, 'override_persons.csv'))\n",
    "        cut_tours.to_csv(os.path.join(crop_output_path, 'override_tours.csv'))\n",
    "        cut_trips.to_csv(os.path.join(crop_output_path, 'override_trips.csv'))\n",
    "        cut_jtp.to_csv(os.path.join(crop_output_path, 'override_joint_tour_participants.csv'))\n",
    "\n",
    "    return zones\n",
    "\n",
    "# failing_in_mtf = [622, 4233, 6192, 6821, 7928, 7968, 13020, 17427, 22830, 41427]\n",
    "# failing_in_mtf_zones = return_relevant_zones(failing_in_mtf)\n",
    "\n",
    "se_households = or_households[(or_households.school_escorting_inbound > 1) | (or_households.school_escorting_inbound > 1)].household_id.values[:40]\n",
    "crop_output_path = r'C:\\ABM3_dev\\run_data\\data_2z_series15_crop_debug'\n",
    "se_zones = return_relevant_zones(se_households, crop_output_path)\n",
    "\n",
    "# output_zones = pd.DataFrame(np.unique(np.concatenate([failing_in_mtf_zones, se_zones])))\n",
    "output_zones = pd.DataFrame(np.unique(se_zones))\n",
    "output_zones.to_csv(os.path.join(final_output_path, 'se_zones.csv'), index=False, header=['MGRA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_output_path = r'C:\\ABM3_dev\\run_data\\data_2z_series15_crop_debug'\n",
    "cut_households = pd.read_csv(os.path.join(crop_output_path, 'override_households.csv'))\n",
    "cut_persons = pd.read_csv(os.path.join(crop_output_path, 'override_persons.csv'))\n",
    "cut_tours = pd.read_csv(os.path.join(crop_output_path, 'override_tours.csv'))\n",
    "cut_trips = pd.read_csv(os.path.join(crop_output_path, 'override_trips.csv'))\n",
    "cut_jtp = pd.read_csv(os.path.join(crop_output_path, 'override_joint_tour_participants.csv'))\n",
    "cut_landuse = pd.read_csv(os.path.join(crop_output_path, 'land_use.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baydag_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "71374a42f9ca49594a59d8cd0cf0a9d6991799261e65149b56c921a17ff5df21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
